{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24182e13-4f90-4085-b10d-80dd9894bb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "import torchaudio\n",
    "from speechbrain.pretrained.interfaces import foreign_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e60fb7-6e58-4b58-a744-9b6f77a9c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "speechbrain.lobes.models.huggingface_wav2vec - wav2vec 2.0 is frozen.\n"
     ]
    }
   ],
   "source": [
    "classifier = foreign_class(source=\"/mntcephfs/lab_data/shoinoue/Models/trained_models/accent/CustomEncoderWav2vec2Classifier-a72df039c801fa14a1c3226e95ab8c14/\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\", savedir=\"/mntcephfs/lab_data/shoinoue/Models/trained_models/accent/CustomEncoderWav2vec2Classifier-a72df039c801fa14a1c3226e95ab8c14/\", run_opts={\"device\":\"cuda\"})\n",
    "accents = [\"us\", \"england\", \"australia\", \"indian\", \"canada\", \"bermuda\", \"scotland\", \"african\", \"ireland\", \"newzealand\", \"wales\", \"malaysia\", \"philippines\", \"singapore\", \"hongkong\", \"southatlandtic\",]\n",
    "\n",
    "def run_classification(path):\n",
    "    audio, _ = librosa.load(path, sr=fs)\n",
    "    torchaudio.save(tempfile, torch.tensor(audio).unsqueeze(0), fs)\n",
    "    signal, org_sr = torchaudio.load(tempfile)\n",
    "    signal = torchaudio.functional.resample(signal, orig_freq=org_sr, new_freq=fs)\n",
    "    outputs =  classifier.encode_batch(signal)\n",
    "    embeddings =  outputs[0].detach().cpu().numpy()\n",
    "    outputs = classifier.mods.output_mlp(outputs)\n",
    "    rawaccent = outputs[0].detach().cpu().numpy()\n",
    "    probaccent = classifier.hparams.softmax(outputs).detach().cpu().numpy()[0]\n",
    "    return embeddings, rawaccent, probaccent\n",
    "\n",
    "def cosine_similarity(e1, e2): # from wespeaker, delete the normalizing part\n",
    "    e1 = torch.tensor(e1)\n",
    "    e2 = torch.tensor(e2)\n",
    "    cosine_score = torch.dot(e1, e2) / (torch.norm(e1) * torch.norm(e2))\n",
    "    cosine_score = cosine_score.item()\n",
    "    return cosine_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62f3bd8-d640-40d3-b05c-1978b8061db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode = \"additional_vctk_1hr\"\n",
    "mode = \"trial_bigvgan\"\n",
    "base_dir = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/\"\n",
    "accent_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/wav/\"\n",
    "native_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/wav/\"\n",
    "fs = 16000\n",
    "\n",
    "labels = {}\n",
    "if mode==\"additional_vctk_1hr\":\n",
    "    labels[\"VTN_fine-tuning_nocondition_gt2syn_melmel_hubert_norepeating_smaller\"] = \"only-gt_small\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_gt2syn_melmel_hubert_norepeating_small\"] = \"only-gt_normal\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_gt2syn_melmel_hubert_norepeating\"] = \"only-gt_large\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_mix2synVCTK1hr_melmel_hubert_norepeating_smaller\"] = \"gtsynVCTK_small\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_mix2synVCTK1hr_melmel_hubert_norepeating_small\"] = \"gtsynVCTK_normal\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_mix2synVCTK1hr_melmel_hubert_norepeating\"] = \"gtsynVCTK_large\"\n",
    "elif mode==\"trial_bigvgan\":\n",
    "    labels[\"VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating_smaller\"] = \"only-gt_small\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating_small\"] = \"only-gt_normal\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating\"] = \"only-gt_large\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating_smaller\"] = \"gtsynVCTK_small\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating_small\"] = \"gtsynVCTK_normal\"\n",
    "    labels[\"VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating\"] = \"gtsynVCTK_large\"\n",
    "base_dir = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d8e7ea-7049-49b0-a542-950ada8bde66",
   "metadata": {},
   "source": [
    "- Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16af6b55-0580-4793-914f-0d9c3c5e8c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating_smaller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2076.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 5966.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 5601.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating_smaller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 5261.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4730.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 4541.01it/s]\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "la = \"Hindi\"\n",
    "tempfile = \"temp.wav\"\n",
    "\n",
    "testsamples = list(np.load(\"data_split_ARCTIC.npy\", allow_pickle=True))[2]\n",
    "for mn in labels:\n",
    "    print(mn)\n",
    "    files = glob.glob(base_dir + mn + \"/100000/wav/*.wav\")\n",
    "    files.sort()\n",
    "    for path in tqdm(files):\n",
    "        basename = os.path.basename(path)[:-4]\n",
    "        accent = accent_dir + basename + \".wav\"\n",
    "        native = native_dir + basename + \".wav\"\n",
    "        save_path = \"/\".join(path.split(\"/\")[:-2])+\"/accent_embeddings/\" + basename + \".npy\"\n",
    "        if not(os.path.exists(save_path)):\n",
    "        # if True:\n",
    "            embeddings, rawaccent, probaccent = run_classification(path)\n",
    "            dirname = os.path.dirname(save_path)\n",
    "            if not(os.path.exists(dirname)):\n",
    "                os.makedirs(dirname, exist_ok=True)\n",
    "            data = {\n",
    "                \"embeddings\": embeddings,\n",
    "                \"raw\": rawaccent,\n",
    "                \"prob\": probaccent\n",
    "            }\n",
    "            np.save(save_path, data)\n",
    "            \n",
    "        accent_path = f\"./evaluation/accent_embeddings/{la}/{basename}.npy\"\n",
    "        native_path = f\"./evaluation/accent_embeddings/English/{basename}.npy\"\n",
    "        paths = [accent_path, native_path]\n",
    "        if not(os.path.exists(accent_path) and os.path.exists(native_path)):\n",
    "        # if True:\n",
    "            for i, path in enumerate([accent, native]):\n",
    "                save_path = paths[i]\n",
    "                embeddings, rawaccent, probaccent = run_classification(path)\n",
    "                dirname = os.path.dirname(save_path)\n",
    "                if not(os.path.exists(dirname)):\n",
    "                    os.makedirs(dirname, exist_ok=True)\n",
    "                data = {\n",
    "                    \"embeddings\": embeddings,\n",
    "                    \"raw\": rawaccent,\n",
    "                    \"prob\": probaccent\n",
    "                }\n",
    "                np.save(save_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4a34c-885b-43ba-aac8-1a6006586212",
   "metadata": {},
   "source": [
    "- Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2eadc2-b757-4002-9e65-2a87612f6845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating_smaller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 619.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 612.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_gt2syn_80mel80mel_hubert_norepeating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 626.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating_smaller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 639.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 622.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTN_fine-tuning_nocondition_mix2synVCTK1hr_80mel80mel_hubert_norepeating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 548.24it/s]\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "la = \"Hindi\"\n",
    "scores = {}\n",
    "testsamples = list(np.load(\"data_split_ARCTIC.npy\", allow_pickle=True))[2]\n",
    "for mn in [\"Hindi\", \"English\"]:\n",
    "    scores[mn] = {}\n",
    "    for key in [\"raw\", \"prob\"]:\n",
    "        scores[mn][key] = []\n",
    "for mn in labels:\n",
    "    scores[mn] = {}\n",
    "    for key in [\"embeddings\", \"raw\", \"prob\"]:\n",
    "        scores[mn][key] = []\n",
    "    print(mn)\n",
    "    files = glob.glob(base_dir + mn + \"/100000/wav/*.wav\")\n",
    "    files.sort()\n",
    "    for path in tqdm(files):\n",
    "        basename = os.path.basename(path)[:-4]\n",
    "        if not(basename in testsamples):\n",
    "            continue\n",
    "        pred_path = \"/\".join(path.split(\"/\")[:-2])+\"/accent_embeddings/\" + basename + \".npy\"\n",
    "        accent_path = f\"./evaluation/accent_embeddings/{la}/{basename}.npy\"\n",
    "        native_path = f\"./evaluation/accent_embeddings/English/{basename}.npy\"\n",
    "        pred = np.load(pred_path, allow_pickle=True).item()\n",
    "        accent = np.load(accent_path, allow_pickle=True).item()\n",
    "        native = np.load(native_path, allow_pickle=True).item()\n",
    "        \n",
    "        key = \"embeddings\"\n",
    "        a = cosine_similarity(pred[key], accent[key])\n",
    "        b = cosine_similarity(pred[key], native[key])\n",
    "        diff = a-b\n",
    "        scores[mn][key] += [diff]\n",
    "        \n",
    "        for key in [\"raw\", \"prob\"]:\n",
    "            scores[mn][key] += [pred[key][3]]\n",
    "            scores[\"Hindi\"][key] += [accent[key][3]]\n",
    "            scores[\"English\"][key] += [native[key][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "409421dd-3c51-4248-9838-4d1ec5533830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification score (before softmax)</th>\n",
       "      <th>classification prob.</th>\n",
       "      <th>AECS ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>13.955</td>\n",
       "      <td>0.819</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>-9.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only-gt_small</th>\n",
       "      <td>16.173</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only-gt_normal</th>\n",
       "      <td>16.507</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only-gt_large</th>\n",
       "      <td>13.174</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gtsynVCTK_small</th>\n",
       "      <td>15.984</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gtsynVCTK_normal</th>\n",
       "      <td>17.595</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gtsynVCTK_large</th>\n",
       "      <td>15.967</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  classification score (before softmax)  classification prob.  \\\n",
       "Hindi                                            13.955                 0.819   \n",
       "English                                          -9.042                 0.000   \n",
       "only-gt_small                                    16.173                 0.898   \n",
       "only-gt_normal                                   16.507                 0.943   \n",
       "only-gt_large                                    13.174                 0.835   \n",
       "gtsynVCTK_small                                  15.984                 0.910   \n",
       "gtsynVCTK_normal                                 17.595                 0.960   \n",
       "gtsynVCTK_large                                  15.967                 0.903   \n",
       "\n",
       "                 AECS ratio  \n",
       "Hindi                        \n",
       "English                      \n",
       "only-gt_small         0.466  \n",
       "only-gt_normal        0.488  \n",
       "only-gt_large         0.436  \n",
       "gtsynVCTK_small       0.453  \n",
       "gtsynVCTK_normal      0.512  \n",
       "gtsynVCTK_large       0.493  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for mn in list(labels.keys())+[\"Hindi\", \"English\"]:\n",
    "    for key in scores[mn]:\n",
    "        scores[mn][key] = np.mean(scores[mn][key])\n",
    "df = pd.DataFrame(scores).T\n",
    "df = np.round(df, 3)\n",
    "df.columns = [\"classification score (before softmax)\", \"classification prob.\", \"AECS ratio\"]\n",
    "df.index = [\"Hindi\", \"English\"] + list(labels.values())\n",
    "df[(1-pd.notna(df)).astype(bool)] = \"\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a72c59-1ec6-4c6e-ac7e-27c7e0a13c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
