{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14ae1fd-ea23-4724-b100-8c789924c3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from pyfiles.dataset import PretrainingMelDataset\n",
    "from pyfiles.utils import Dict2Obj\n",
    "from pyfiles.feature_extractor import get_vocos\n",
    "\n",
    "import seq2seq_vc.models\n",
    "from seq2seq_vc.utils import read_hdf5, write_hdf5\n",
    "from seq2seq_vc.utils.plot import plot_attention, plot_generated_and_ref_2d, plot_1d\n",
    "from seq2seq_vc.utils.types import str2bool\n",
    "from seq2seq_vc.utils.duration_calculator import DurationCalculator\n",
    "\n",
    "sys.path.append(\"../../cuhksz-phd/sho_util/pyfiles/\")\n",
    "from pytorch import cuda2numpy, cuda2cpu\n",
    "from basic import plot_spectrogram\n",
    "from sound import play_audio\n",
    "\n",
    "import joblib\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46d6bda-3a0e-4dd9-bcf4-9d1d60f8a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install --no-deps torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !pip install --no-deps torchaudio==0.10.0 -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !pip install paddlepaddle-gpu==2.4.2 -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70a307d-1a6a-4fb4-92d8-10812763aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Variables\n",
    "dataset_dir = \"/mntcephfs/lee_dataset/tts/LibriTTS_R/\"\n",
    "# feat_base_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/LibriTTS_R/features/\"\n",
    "feat_base_dir = \"/mntcephfs/data/audiow/shoinoue/Dataset/LibriTTS_R/features/\"\n",
    "\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R_16000.save\"\n",
    "# scaler_filename = f\"ckpts/scalers/LibriTTS-R.save\" # if model size is smaller\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b63de03-513d-4559-92fb-5e96cb8f6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_small/checkpoint-400000steps.pkl\"\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_fromhubert_smaller/checkpoint-50000steps.pkl\"\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_fromhubert_small/checkpoint-200000steps.pkl\"\n",
    "\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_fromhubert_norepeating_smaller/checkpoint-100000steps.pkl\"\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_fromhubert_norepeating_small/checkpoint-50000steps.pkl\"\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_fromhubert_norepeating/checkpoint-100000steps.pkl\"\n",
    "\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_from1000hubert_norepeating_small/checkpoint-50000steps.pkl\"\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_from1000hubert_norepeating/checkpoint-100000steps.pkl\"\n",
    "\n",
    "checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_from300hubert_norepeating/checkpoint-100000steps.pkl\"\n",
    "\n",
    "if \"smaller\" in checkpoint_path:\n",
    "    size = \"smaller\"\n",
    "elif \"small\" in checkpoint_path:\n",
    "    size = \"small\"\n",
    "else:\n",
    "    size = \"\"\n",
    "\n",
    "args = {}\n",
    "args[\"rank\"] = 0\n",
    "args[\"checkpoint\"] = checkpoint_path\n",
    "args[\"outdir\"] = os.path.dirname(args[\"checkpoint\"]) + \"/\"\n",
    "args[\"config_path\"] = f\"./../egs/l2-arctic/cascade/conf/{size}m2mvtn.melmel.yaml\"\n",
    "args[\"init_checkpoint\"] = \"\"\n",
    "args[\"resume\"] = \"\"\n",
    "args[\"distributed\"] = False\n",
    "args = Dict2Obj(args)\n",
    "\n",
    "# load main config\n",
    "with open(args.config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "config.update(vars(args))\n",
    "\n",
    "# Customization\n",
    "if \"addition\" in checkpoint_path:\n",
    "    config[\"model_params\"][\"conditiontype\"] = \"add\"\n",
    "elif \"concatenation\" in checkpoint_path:\n",
    "    config[\"model_params\"][\"conditiontype\"] = \"concat\"\n",
    "elif \"nocondition\" in checkpoint_path:\n",
    "    config[\"model_params\"][\"conditiontype\"] = \"nocondition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97a500d-fc4b-46cf-a782-224a95dd6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_device(args.rank)\n",
    "    \n",
    "### Dataset Preparation ###\n",
    "dataset = PretrainingMelDataset(feat_base_dir, dataset_dir, [scaler,scaler], \"test\", input_output_type=[\"mel\", \"mel\"], defiling_ratio=[0,0,1], noembedding=True)\n",
    "# Get only the short audio\n",
    "# target_length = 150\n",
    "# lengths = []\n",
    "# for i in tqdm(range(len(dataset))):\n",
    "#     length = dataset[i][\"src_feat\"].shape[0]\n",
    "#     lengths += [length]\n",
    "# bool_list = np.array(lengths)<target_length\n",
    "# dataset.ifiles = list(np.array(dataset.ifiles)[bool_list])\n",
    "# dataset.files = list(np.array(dataset.files)[bool_list])\n",
    "\n",
    "### Model Preparation ###\n",
    "model_class = getattr(seq2seq_vc.models, config[\"model_type\"])\n",
    "model = model_class(**config[\"model_params\"])\n",
    "model.load_state_dict(torch.load(args.checkpoint, map_location=\"cpu\")[\"model\"])\n",
    "model = model.eval().to(device)\n",
    "    \n",
    "### Vocoder Preparation ###\n",
    "data_dir = \"/mntcephfs/lab_data/shoinoue/\"\n",
    "# fs = 24000\n",
    "fs = 16000\n",
    "\n",
    "if fs==24000:\n",
    "    config_path = f\"{data_dir}Models/trained_models/vocos/24k/config.yaml\"\n",
    "    model_path = f\"{data_dir}Models/trained_models/vocos/24k/pytorch_model.bin\"\n",
    "elif fs==16000:\n",
    "    config_path = f\"{data_dir}Models/trained_models/vocos/vocos16k_noncausal_tealab/config16k.yaml\"\n",
    "    model_path = f\"{data_dir}Models/trained_models/vocos/vocos16k_noncausal_tealab/vocos16k_noncausal_last.ckpt\"\n",
    "vocoder = get_vocos(config_path, model_path, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e959a0e9-ad04-49a9-b9b4-66a593960cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[decode]:   0%|          | 1/4837 [00:17<23:09:35, 17.24s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(), tqdm(dataset, desc=\"[decode]\") as pbar:\n",
    "    for idx, batch in enumerate(pbar, 1):\n",
    "        start_time = time.time()\n",
    "        utt_id = os.path.basename(dataset.files[idx-1]).split(\".\")[0]\n",
    "        x = batch[\"src_feat\"]\n",
    "        x = torch.tensor(x, dtype=torch.float).to(device)\n",
    "        xembs = torch.tensor(batch[\"src_condition\"].reshape(1, -1), dtype=torch.float).to(device)\n",
    "        yembs = torch.tensor(batch[\"trg_condition\"].reshape(1, -1), dtype=torch.float).to(device)\n",
    "        outs, probs, att_ws = model.inference(\n",
    "            x, config[\"inference\"], xembs, yembs, spemb=None\n",
    "        )\n",
    "\n",
    "        # plot figures\n",
    "        plot_generated_and_ref_2d(\n",
    "            outs.cpu().numpy(),\n",
    "            config[\"outdir\"] + f\"/outs/{utt_id}.png\",\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "        \n",
    "        plot_1d(\n",
    "            probs.cpu().numpy(),\n",
    "            config[\"outdir\"] + f\"/probs/{utt_id}_prob.png\",\n",
    "        )\n",
    "        plot_attention(\n",
    "            att_ws.cpu().numpy(),\n",
    "            config[\"outdir\"] + f\"/att_ws/{utt_id}_att_ws.png\",\n",
    "        )\n",
    "        \n",
    "        ### Waveform ###\n",
    "        if not os.path.exists(os.path.join(config[\"outdir\"], \"wav\")):\n",
    "            os.makedirs(os.path.join(config[\"outdir\"], \"wav\"), exist_ok=True)\n",
    "        denormalized_mel = scaler.inverse_transform(cuda2numpy(outs))\n",
    "        y = vocoder.decode(torch.tensor(denormalized_mel.T).unsqueeze(0))\n",
    "        sf.write(\n",
    "            os.path.join(config[\"outdir\"], \"wav\", f\"{utt_id}.wav\"),\n",
    "            y.cpu().numpy()[0],\n",
    "            fs,\n",
    "            \"PCM_16\",\n",
    "        )\n",
    "        \n",
    "        ### Mel ###\n",
    "        if not os.path.exists(os.path.join(config[\"outdir\"], \"mel\")):\n",
    "            os.makedirs(os.path.join(config[\"outdir\"], \"mel\"), exist_ok=True)\n",
    "            \n",
    "        denormalized_input_mel = scaler.inverse_transform(x.cpu().numpy())\n",
    "        np.save(os.path.join(config[\"outdir\"], \"mel\", f\"output_{utt_id}.npy\"), denormalized_mel)\n",
    "        np.save(os.path.join(config[\"outdir\"], \"mel\", f\"input_{utt_id}.npy\"), denormalized_input_mel)\n",
    "        np.save(os.path.join(config[\"outdir\"], \"probs\", f\"probs_{utt_id}.npy\"), probs.cpu().numpy())\n",
    "        np.save(os.path.join(config[\"outdir\"], \"att_ws\", f\"attws_{utt_id}.npy\"), att_ws.cpu().numpy())\n",
    "        if idx>2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fac35a-72de-4c9d-ae53-dcf788a77126",
   "metadata": {},
   "source": [
    "# Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc1948c-1608-424b-b362-f64fce346ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_id = dataset.files[0]\n",
    "utt_basename = os.path.basename(utt_id)[:-4]\n",
    "data = dataset[dataset.files.index(utt_id)]\n",
    "\n",
    "imelpath = os.path.join(config[\"outdir\"], \"mel\", f\"input_{utt_basename}.npy\")\n",
    "omelpath = os.path.join(config[\"outdir\"], \"mel\", f\"output_{utt_basename}.npy\")\n",
    "wavpath = os.path.join(config[\"outdir\"], \"wav\", f\"{utt_basename}.wav\")\n",
    "predmel = np.load(omelpath)\n",
    "predwav, sr = torchaudio.load(wavpath)\n",
    "gtmel = data[\"trg_feat\"]\n",
    "gtmel = scaler.inverse_transform(gtmel)\n",
    "gtwav = vocoder.decode(torch.tensor(gtmel.T).unsqueeze(0))\n",
    "sourcemel = np.load(imelpath)\n",
    "sourcewav = vocoder.decode(torch.tensor(sourcemel.T).unsqueeze(0))\n",
    "\n",
    "%matplotlib inline  \n",
    "fig = plt.figure(figsize=(18, 15))\n",
    "print(\"source\")\n",
    "play_audio(sourcewav, fs)\n",
    "print(\"ground truth\")\n",
    "play_audio(gtwav, fs)\n",
    "print(\"predicted\")\n",
    "play_audio(predwav, sr)\n",
    "\n",
    "plot_spectrogram(sourcemel.T, fig, (2,3,1), title=\"source (masked)\")\n",
    "plot_spectrogram(gtmel.T, fig, (2,3,2), title=\"ground truth\")\n",
    "plot_spectrogram(predmel.T, fig, (2,3,3), title=\"reconstructed\")\n",
    "\n",
    "probs = np.load(os.path.join(config[\"outdir\"], \"probs\", f\"probs_{utt_basename}.npy\"))\n",
    "attws = np.load(os.path.join(config[\"outdir\"], \"att_ws\", f\"attws_{utt_basename}.npy\"))\n",
    "att = np.array([attws[i][j] for i in range(attws.shape[0]) for j in range(attws.shape[1])]).mean(0)\n",
    "plot_spectrogram(att, fig, (2,3,4), title=\"attention weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ed0c0-9965-4621-a688-7cccdd3ab2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4593cc4-28a4-45b3-8cf3-a16052683084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8cb52-9f48-4efb-a21e-3c8ba88b8803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
