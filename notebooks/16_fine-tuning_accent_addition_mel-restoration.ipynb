{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14ae1fd-ea23-4724-b100-8c789924c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pyfiles.dataset import ParallelArcticDataset\n",
    "\n",
    "import seq2seq_vc\n",
    "import seq2seq_vc.models\n",
    "import seq2seq_vc.losses\n",
    "import seq2seq_vc.trainers\n",
    "import seq2seq_vc.collaters\n",
    "\n",
    "# from seq2seq_vc.datasets import ParallelVCMelDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from seq2seq_vc.utils import read_hdf5\n",
    "from seq2seq_vc.utils.types import str_or_none\n",
    "\n",
    "# set to avoid matplotlib error in CLI environment\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seq2seq_vc.schedulers.warmup_lr import WarmupLR\n",
    "\n",
    "scheduler_classes = dict(warmuplr=WarmupLR)\n",
    "\n",
    "class Dict2Obj(object):\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])\n",
    "\n",
    "import joblib\n",
    "import glob\n",
    "datasplit = list(np.load(\"./data_split_ARCTIC.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa70bbd0-583d-4103-926f-df7863294597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Variables\n",
    "src_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"\n",
    "trg_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"\n",
    "\n",
    "scaler = {}\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R_16000.save\"\n",
    "scaler[\"mel\"] = joblib.load(scaler_filename)\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R_wavlm.save\"\n",
    "scaler[\"wavlm\"] = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64ecf55-0e57-4891-9c20-68169f2224c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/pretraining_nocondition_LibriTTS-R_melmel_small/checkpoint-500000steps.pkl\"\n",
    "\n",
    "if \"smaller\" in checkpoint_path:\n",
    "    size = \"smaller\"\n",
    "elif \"small\" in checkpoint_path:\n",
    "    size = \"small\"\n",
    "else:\n",
    "    size = \"\"\n",
    "if \"addition\" in checkpoint_path:\n",
    "    conditiontype = \"add\"\n",
    "elif \"concatenation\" in checkpoint_path:\n",
    "    conditiontype = \"concat\"\n",
    "elif \"nocondition\" in checkpoint_path:\n",
    "    conditiontype = \"nocondition\"\n",
    "\n",
    "args = {}\n",
    "args[\"rank\"] = 0\n",
    "args[\"outdir\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/fine-tuning_{conditiontype}_accentaddition_gt2syn_melmel_{size}_trainabledecoder_cutsilence/\"\n",
    "args[\"config_path\"] = f\"./../egs/l2-arctic/cascade/conf/{size}m2mvtn.melmel.yaml\"\n",
    "args[\"init_checkpoint\"] = checkpoint_path\n",
    "args[\"resume\"] = \"\"\n",
    "args[\"distributed\"] = False\n",
    "args = Dict2Obj(args)\n",
    "\n",
    "# load main config\n",
    "with open(args.config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "config.update(vars(args))\n",
    "inputoutput = [\"mel\", \"mel\"]\n",
    "\n",
    "config[\"model_params\"][\"conditiontype\"] = conditiontype\n",
    "# config[\"freeze-mods\"] = [\"decoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73d4fbe-2fe1-4791-832a-8cbbc6e37984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Overriding module encoder.embed.conv.0.weight\n",
      "WARNING:root:Overriding module encoder.embed.conv.0.bias\n",
      "WARNING:root:Overriding module encoder.embed.conv.2.weight\n",
      "WARNING:root:Overriding module encoder.embed.conv.2.bias\n",
      "WARNING:root:Overriding module encoder.embed.out.0.weight\n",
      "WARNING:root:Overriding module encoder.embed.out.0.bias\n",
      "WARNING:root:Overriding module encoder.embed.out.1.alpha\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm2.bias\n",
      "WARNING:root:Overriding module encoder.after_norm.weight\n",
      "WARNING:root:Overriding module encoder.after_norm.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.0.0.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.0.0.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.1.0.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.1.0.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.1.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.1.bias\n",
      "WARNING:root:Overriding module decoder.embed.1.alpha\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm3.bias\n",
      "WARNING:root:Overriding module feat_out.weight\n",
      "WARNING:root:Overriding module feat_out.bias\n",
      "WARNING:root:Overriding module prob_out.weight\n",
      "WARNING:root:Overriding module prob_out.bias\n",
      "WARNING:root:Overriding module postnet.postnet.0.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.1.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.2.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.3.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.4.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.num_batches_tracked\n",
      "WARNING:root:Overriding module condition_encoding_embedding.weight\n",
      "WARNING:root:Overriding module condition_encoding_embedding.bias\n",
      "WARNING:root:Overriding module condition_decoding_embedding.weight\n",
      "WARNING:root:Overriding module condition_decoding_embedding.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_device(args.rank)\n",
    "if not os.path.exists(args.outdir):\n",
    "    os.makedirs(args.outdir)\n",
    "    \n",
    "### Dataset Preparation ###\n",
    "dataset = {\n",
    "    \"train\": ParallelArcticDataset(src_dir, trg_dir, datasplit, scaler, \"train\", input_output=inputoutput),\n",
    "    \"dev\": ParallelArcticDataset(src_dir, trg_dir, datasplit, scaler, \"valid\", input_output=inputoutput),\n",
    "}\n",
    "\n",
    "collater_class = getattr(\n",
    "    seq2seq_vc.collaters,\n",
    "    config.get(\"collater_type\", \"ARM2MVCCollater\"),\n",
    ")\n",
    "collater = collater_class()\n",
    "\n",
    "sampler = {\"train\": None, \"dev\": None}\n",
    "data_loader = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=dataset[\"train\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"train\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "    \"dev\": DataLoader(\n",
    "        dataset=dataset[\"dev\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"dev\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "}\n",
    "\n",
    "### Model Preparation ###\n",
    "model_class = getattr(\n",
    "    seq2seq_vc.models,\n",
    "    config.get(\"model_type\", \"M2MVTN\"),\n",
    ")\n",
    "model = model_class(**config[\"model_params\"]).to(device)\n",
    "\n",
    "if config.get(\"criterions\", None):\n",
    "    criterion = {\n",
    "        criterion_class: getattr(seq2seq_vc.losses, criterion_class)(\n",
    "            **criterion_paramaters\n",
    "        )\n",
    "        for criterion_class, criterion_paramaters in config[\"criterions\"].items()\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"Please specify criterions in the config file.\")\n",
    "\n",
    "### optimizers and schedulers ###\n",
    "optimizer_class = getattr(\n",
    "    torch.optim,\n",
    "    # keep compatibility\n",
    "    config.get(\"optimizer_type\", \"Adam\"),\n",
    ")\n",
    "optimizer = optimizer_class(\n",
    "    model.parameters(),\n",
    "    **config[\"optimizer_params\"],\n",
    ")\n",
    "scheduler_class = scheduler_classes.get(config.get(\"scheduler_type\", \"warmuplr\"))\n",
    "scheduler = scheduler_class(\n",
    "    optimizer=optimizer,\n",
    "    **config[\"scheduler_params\"],\n",
    ")\n",
    "\n",
    "### define trainer ###\n",
    "trainer_class = getattr(\n",
    "    seq2seq_vc.trainers,\n",
    "    config.get(\"trainer_type\", \"ARM2MVCTrainer\"),\n",
    ")\n",
    "trainer = trainer_class(\n",
    "    steps=0,\n",
    "    epochs=0,\n",
    "    data_loader=data_loader,\n",
    "    sampler=sampler,\n",
    "    model=model,\n",
    "    vocoder=None,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# load pretrained parameters from checkpoint\n",
    "if len(args.init_checkpoint) != 0:\n",
    "    trainer.load_trained_modules(\n",
    "        args.init_checkpoint, init_mods=config[\"init-mods\"]\n",
    "    )\n",
    "\n",
    "# resume from checkpoint\n",
    "if len(args.resume) != 0:\n",
    "    trainer.load_checkpoint(args.resume)\n",
    "\n",
    "# freeze modules if necessary\n",
    "if config.get(\"freeze-mods\", None) is not None:\n",
    "    assert type(config[\"freeze-mods\"]) is list\n",
    "    trainer.freeze_modules(config[\"freeze-mods\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdc6c57-cf10-42d2-a759-b9f28a9a1811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train]:   0%|          | 0/500000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 704.00 MiB (GPU 0; 31.74 GiB total capacity; 146.12 MiB already allocated; 688.88 MiB free; 172.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_checkpoint(\n\u001b[1;32m      5\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutdir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msteps.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:76\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtqdm \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m     72\u001b[0m     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_max_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[train]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# train one epoch\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# check whether training is finished\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinish_train:\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:134\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train model one epoch.\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_steps_per_epoch, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# train one step\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/ar_m2mvc.py:82\u001b[0m, in \u001b[0;36mARM2MVCTrainer._train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     71\u001b[0m yembs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myembs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# model forward\u001b[39;00m\n\u001b[1;32m     74\u001b[0m (\n\u001b[1;32m     75\u001b[0m     after_outs,\n\u001b[1;32m     76\u001b[0m     before_outs,\n\u001b[1;32m     77\u001b[0m     logits,\n\u001b[1;32m     78\u001b[0m     ys_,\n\u001b[1;32m     79\u001b[0m     labels_,\n\u001b[1;32m     80\u001b[0m     olens_,\n\u001b[1;32m     81\u001b[0m     (att_ws, ilens_ds_st, olens_in),\n\u001b[0;32m---> 82\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43milens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43molens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxembs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myembs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# seq2seq loss\u001b[39;00m\n\u001b[1;32m     85\u001b[0m l1_loss, bce_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeq2SeqLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[1;32m     86\u001b[0m     after_outs, before_outs, logits, ys_, labels_, olens_\n\u001b[1;32m     87\u001b[0m )\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/models/m2m_vtn.py:238\u001b[0m, in \u001b[0;36mM2MVTN.forward\u001b[0;34m(self, xs, ilens, ys, labels, olens, xembs, yembs, spembs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditiontype\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnocondition\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m hs, hs_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# integrate speaker embedding\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspk_embed_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/modules/transformer/encoder.py:299\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, xs, masks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode input sequence.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed,\n\u001b[1;32m    297\u001b[0m     (Conv2dSubsampling, Conv2dSubsampling6, Conv2dSubsampling8),\n\u001b[1;32m    298\u001b[0m ):\n\u001b[0;32m--> 299\u001b[0m     xs, masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(xs)\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/modules/transformer/subsampling.py:89\u001b[0m, in \u001b[0;36mConv2dSubsampling.forward\u001b[0;34m(self, x, x_mask)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Subsample x.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (b, c, t, f)\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m b, c, t, f \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(b, t, c \u001b[38;5;241m*\u001b[39m f))\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/conv.py:439\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    437\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    438\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 704.00 MiB (GPU 0; 31.74 GiB total capacity; 146.12 MiB already allocated; 688.88 MiB free; 172.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.run()\n",
    "finally:\n",
    "    trainer.save_checkpoint(\n",
    "        os.path.join(config[\"outdir\"], f\"checkpoint-{trainer.steps}steps.pkl\")\n",
    "    )\n",
    "    logging.info(f\"Successfully saved checkpoint @ {trainer.steps}steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4115c-316f-43ae-9c58-04087014a0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5040d2f-3e92-4863-81c7-58641b37e4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
