{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ae1fd-ea23-4724-b100-8c789924c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import seq2seq_vc\n",
    "import seq2seq_vc.models\n",
    "import seq2seq_vc.losses\n",
    "import seq2seq_vc.trainers\n",
    "import seq2seq_vc.collaters\n",
    "\n",
    "# from seq2seq_vc.datasets import ParallelVCMelDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from seq2seq_vc.utils import read_hdf5\n",
    "from seq2seq_vc.utils.types import str_or_none\n",
    "# from seq2seq_vc.vocoder import Vocoder\n",
    "# from seq2seq_vc.vocoder.s3prl_feat2wav import S3PRL_Feat2Wav\n",
    "# from seq2seq_vc.vocoder.griffin_lim import Spectrogram2Waveform\n",
    "# from seq2seq_vc.vocoder.encodec import EnCodec_decoder\n",
    "\n",
    "# set to avoid matplotlib error in CLI environment\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seq2seq_vc.schedulers.warmup_lr import WarmupLR\n",
    "\n",
    "scheduler_classes = dict(warmuplr=WarmupLR)\n",
    "\n",
    "class Dict2Obj(object):\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])\n",
    "\n",
    "import joblib\n",
    "import glob\n",
    "datasplit = list(np.load(\"./data_split_ARCTIC.npy\", allow_pickle=True))\n",
    "\n",
    "class ParallelVCMelDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, speakers, src_spk, trg_spk, datasplit, scaler, mode=\"train\"):\n",
    "        modefiles = datasplit[[\"train\", \"valid\", \"test\"].index(mode)]\n",
    "        filenames = [os.path.basename(a)[:-4] for a in glob.glob(dataset_dir+f\"{speakers[0]}/mel/*\")]\n",
    "        filenames.sort()\n",
    "        files = []\n",
    "        for fn in filenames:\n",
    "            if fn in modefiles:\n",
    "                exist = True\n",
    "                for spk in speakers[1:]:\n",
    "                    if not(os.path.exists(dataset_dir + f\"{spk}/mel/{fn}.npy\")):\n",
    "                        exist = False\n",
    "                        break\n",
    "                if exist:\n",
    "                    files += [fn]\n",
    "        data = {}\n",
    "        for spk in speakers:\n",
    "            data[spk] = [dataset_dir + f\"{spk}/mel/{fn}.npy\" for fn in files]\n",
    "            \n",
    "        self.data = data\n",
    "        self.src_spk = src_spk\n",
    "        self.trg_spk = trg_spk\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[self.src_spk])\n",
    "    def __getitem__(self, idx):\n",
    "        items = {}\n",
    "        items[\"src_feat\"] = self.scaler.transform(np.load(self.data[self.src_spk][idx]).T)\n",
    "        items[\"trg_feat\"] = self.scaler.transform(np.load(self.data[self.trg_spk][idx]).T)\n",
    "        \n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a307d-1a6a-4fb4-92d8-10812763aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Variables\n",
    "dataset_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/L2-ARCTIC/\"\n",
    "speakers = [\"HKK\", \"TNI\"]\n",
    "src_spk = \"HKK\"\n",
    "trg_spk = \"TNI\"\n",
    "\n",
    "scaler_filename = f\"ckpts/scalers/HKK_TNI.save\"\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ecf55-0e57-4891-9c20-68169f2224c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"rank\"] = 0\n",
    "args[\"outdir\"] = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts/HKK_TNI/\"\n",
    "# args.config_path = \"./../egs/l2-arctic/cascade/conf/vtn.tts_pt.v1.yaml\"\n",
    "args[\"config_path\"] = \"./../egs/arctic/vc1/conf/vtn.v1.melmel.yaml\"\n",
    "args[\"init_checkpoint\"] = \"\"\n",
    "args[\"resume\"] = \"\"\n",
    "args[\"distributed\"] = False\n",
    "args = Dict2Obj(args)\n",
    "\n",
    "# load main config\n",
    "with open(args.config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "config.update(vars(args))\n",
    "\n",
    "# Customization\n",
    "config[\"model_params\"][\"idim\"] = 100\n",
    "config[\"model_params\"][\"odim\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d4fbe-2fe1-4791-832a-8cbbc6e37984",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speakers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(args\u001b[38;5;241m.\u001b[39moutdir)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m### Dataset Preparation ###\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: ParallelVCMelDataset(dataset_dir, \u001b[43mspeakers\u001b[49m, src_spk, trg_spk, datasplit, scaler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m: ParallelVCMelDataset(dataset_dir, speakers, src_spk, trg_spk, datasplit, scaler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m collater_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m     14\u001b[0m     seq2seq_vc\u001b[38;5;241m.\u001b[39mcollaters,\n\u001b[1;32m     15\u001b[0m     config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollater_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mARVCCollater\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m collater \u001b[38;5;241m=\u001b[39m collater_class()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'speakers' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_device(args.rank)\n",
    "if not os.path.exists(args.outdir):\n",
    "    os.makedirs(args.outdir)\n",
    "    \n",
    "### Dataset Preparation ###\n",
    "dataset = {\n",
    "    \"train\": ParallelVCMelDataset(dataset_dir, speakers, src_spk, trg_spk, datasplit, scaler, \"train\"),\n",
    "    \"dev\": ParallelVCMelDataset(dataset_dir, speakers, src_spk, trg_spk, datasplit, scaler, \"valid\"),\n",
    "}\n",
    "\n",
    "collater_class = getattr(\n",
    "    seq2seq_vc.collaters,\n",
    "    config.get(\"collater_type\", \"ARVCCollater\"),\n",
    ")\n",
    "collater = collater_class()\n",
    "\n",
    "sampler = {\"train\": None, \"dev\": None}\n",
    "data_loader = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=dataset[\"train\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"train\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "    \"dev\": DataLoader(\n",
    "        dataset=dataset[\"dev\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"dev\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "}\n",
    "\n",
    "### Model Preparation ###\n",
    "model_class = getattr(\n",
    "    seq2seq_vc.models,\n",
    "    config.get(\"model_type\", \"VTN\"),\n",
    ")\n",
    "model = model_class(**config[\"model_params\"]).to(device)\n",
    "\n",
    "if config.get(\"criterions\", None):\n",
    "    criterion = {\n",
    "        criterion_class: getattr(seq2seq_vc.losses, criterion_class)(\n",
    "            **criterion_paramaters\n",
    "        )\n",
    "        for criterion_class, criterion_paramaters in config[\"criterions\"].items()\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"Please specify criterions in the config file.\")\n",
    "\n",
    "### optimizers and schedulers ###\n",
    "optimizer_class = getattr(\n",
    "    torch.optim,\n",
    "    # keep compatibility\n",
    "    config.get(\"optimizer_type\", \"Adam\"),\n",
    ")\n",
    "optimizer = optimizer_class(\n",
    "    model.parameters(),\n",
    "    **config[\"optimizer_params\"],\n",
    ")\n",
    "scheduler_class = scheduler_classes.get(config.get(\"scheduler_type\", \"warmuplr\"))\n",
    "scheduler = scheduler_class(\n",
    "    optimizer=optimizer,\n",
    "    **config[\"scheduler_params\"],\n",
    ")\n",
    "\n",
    "### define trainer ###\n",
    "trainer_class = getattr(\n",
    "    seq2seq_vc.trainers,\n",
    "    config.get(\"trainer_type\", \"ARVCTrainer\"),\n",
    ")\n",
    "trainer = trainer_class(\n",
    "    steps=0,\n",
    "    epochs=0,\n",
    "    data_loader=data_loader,\n",
    "    sampler=sampler,\n",
    "    model=model,\n",
    "    vocoder=None,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# load pretrained parameters from checkpoint\n",
    "if len(args.init_checkpoint) != 0:\n",
    "    trainer.load_trained_modules(\n",
    "        args.init_checkpoint, init_mods=config[\"init-mods\"]\n",
    "    )\n",
    "\n",
    "# resume from checkpoint\n",
    "if len(args.resume) != 0:\n",
    "    trainer.load_checkpoint(args.resume)\n",
    "\n",
    "# freeze modules if necessary\n",
    "if config.get(\"freeze-mods\", None) is not None:\n",
    "    assert type(config[\"freeze-mods\"]) is list\n",
    "    trainer.freeze_modules(config[\"freeze-mods\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc6c57-cf10-42d2-a759-b9f28a9a1811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train]:   0%|          | 10/50000 [00:23<26:35:47,  1.92s/it][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 20/50000 [00:37<16:43:28,  1.20s/it][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 30/50000 [00:50<14:04:19,  1.01s/it][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 40/50000 [00:59<12:11:58,  1.14it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 50/50000 [01:10<13:05:53,  1.06it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 60/50000 [01:20<13:52:16,  1.00it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 70/50000 [01:28<10:37:57,  1.30it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 80/50000 [01:36<10:27:37,  1.33it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 90/50000 [01:44<9:52:04,  1.40it/s] [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 100/50000 [01:52<10:37:53,  1.30it/s]\n",
      "[eval]:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "\n",
      "[eval]: 100%|██████████| 1/1 [00:43<00:00, 43.67s/it]\u001b[A\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 110/50000 [02:44<15:33:24,  1.12s/it] [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 120/50000 [02:51<8:06:56,  1.71it/s] [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 130/50000 [03:01<11:10:22,  1.24it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[train]:   0%|          | 132/50000 [03:03<13:14:21,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.run()\n",
    "finally:\n",
    "    trainer.save_checkpoint(\n",
    "        os.path.join(config[\"outdir\"], f\"checkpoint-{trainer.steps}steps.pkl\")\n",
    "    )\n",
    "    logging.info(f\"Successfully saved checkpoint @ {trainer.steps}steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9ba81-4a85-4efc-930c-1d6d174f0524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44699c-9ffe-4528-a760-3033f8515fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80151273-3fbc-4272-a460-917c027f0ea9",
   "metadata": {},
   "source": [
    "- Output of ParallelVCMelDataset\n",
    "    - src_feat\n",
    "    - trg_feat\n",
    "    - dp_input # duration predictor's input\n",
    "    - duration # no need\n",
    "    - utt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141604c-1d67-498f-a05a-4ee3ffffd54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
