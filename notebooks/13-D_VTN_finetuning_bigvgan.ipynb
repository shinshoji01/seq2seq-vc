{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14ae1fd-ea23-4724-b100-8c789924c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../cuhksz-phd/sho_util/pyfiles/\")\n",
    "from basic import plot_spectrogram\n",
    "from sound import play_audio\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pyfiles.dataset import Parallelo2oVCMelDataset, ParallelArcticDataset\n",
    "\n",
    "import seq2seq_vc\n",
    "import seq2seq_vc.models\n",
    "import seq2seq_vc.losses\n",
    "import seq2seq_vc.trainers\n",
    "import seq2seq_vc.collaters\n",
    "\n",
    "from seq2seq_vc.losses import GuidedMultiHeadAttentionLoss\n",
    "\n",
    "# from seq2seq_vc.datasets import ParallelVCMelDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from seq2seq_vc.utils import read_hdf5\n",
    "from seq2seq_vc.utils.types import str_or_none\n",
    "\n",
    "# set to avoid matplotlib error in CLI environment\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seq2seq_vc.schedulers.warmup_lr import WarmupLR\n",
    "\n",
    "scheduler_classes = dict(warmuplr=WarmupLR)\n",
    "\n",
    "class Dict2Obj(object):\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])\n",
    "\n",
    "import joblib\n",
    "import glob\n",
    "datasplitARCTIC = list(np.load(\"./data_split_ARCTIC.npy\", allow_pickle=True))\n",
    "datasplitVCTK = list(np.load(\"./data_split_VCTK.npy\", allow_pickle=True))\n",
    "datasplit = []\n",
    "for i in range(3):\n",
    "    datasplit += [datasplitARCTIC[i]+datasplitVCTK[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4889c10a-fa76-4f52-b646-c0799e88c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Variables\n",
    "setup = 10\n",
    "\n",
    "if setup==0: ### One-to-one Accent Addition ###\n",
    "    src_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"\n",
    "    trg_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"\n",
    "    setupname = \"gt2syn\"\n",
    "    \n",
    "elif setup==1: ### Many-to-Many Accent Removal ###\n",
    "    spks_dataset = [\"ASI\", \"RRBI\", \"SVBI\", \"TNI\"]\n",
    "    src_dir = [f\"/mntcephfs/lab_data/shoinoue/Dataset/L2-ARCTIC/{spk}/\" for spk in spks_dataset]\n",
    "    trg_dir = [f\"/mntcephfs/data/audiow/shoinoue/Dataset/CosyVoice/{spk}/English/\" for spk in spks_dataset]\n",
    "    setupname = \"gt2synAR\"\n",
    "    \n",
    "elif setup==2: ### Many-to-Many Accent Addition with mixed input (syn and gt) ###\n",
    "    spks_dataset = [\"ASI\", \"RRBI\", \"SVBI\", \"TNI\"]\n",
    "    rep_num = len(spks_dataset)\n",
    "    src_dir = [f\"/mntcephfs/data/audiow/shoinoue/Dataset/CosyVoice/{spk}/English/\" for spk in spks_dataset]\n",
    "    trg_dir = [f\"/mntcephfs/lab_data/shoinoue/Dataset/L2-ARCTIC/{spk}/\" for spk in spks_dataset]\n",
    "    for _ in range(rep_num):\n",
    "        src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"]\n",
    "        trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"]\n",
    "    setupname = \"mix2mixAA\"\n",
    "    \n",
    "elif setup==3: ### Many-to-Many Accent Addition with Synthetic output data using CosyVoice ###\n",
    "    spks_dataset = [\"SLT\", \"BDL\", \"EEY\", \"RMS\", \"AEW\", \"CLB\", \"LJM\", \"LNH\"]\n",
    "    rep_num = len(spks_dataset)\n",
    "    src_dir = [f\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/{spk}/\" for spk in spks_dataset]\n",
    "    trg_dir = [f\"/mntcephfs/data/audiow/shoinoue/Dataset/CosyVoice/{spk}/ASI/\" for spk in spks_dataset]\n",
    "    for _ in range(rep_num):\n",
    "        src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"]\n",
    "        trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"]\n",
    "    setupname = \"gt2synAAcosy\"\n",
    "    \n",
    "elif setup==4: ### Many-to-Many Accent Removal with Synthetic output data using CosyVoice ###\n",
    "    spks_dataset = [\"SLT\", \"BDL\", \"EEY\", \"RMS\", \"AEW\", \"CLB\", \"LJM\", \"LNH\"]\n",
    "    rep_num = len(spks_dataset)\n",
    "    src_dir = [f\"/mntcephfs/data/audiow/shoinoue/Dataset/CosyVoice/{spk}/ASI/\" for spk in spks_dataset]\n",
    "    trg_dir = [f\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/{spk}/\" for spk in spks_dataset]\n",
    "    for _ in range(rep_num):\n",
    "        src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"]\n",
    "        trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"]\n",
    "    setupname = \"syn2gtARcosy\"\n",
    "    \n",
    "if setup==5: ### One-to-one Accent Addition from syn to syn ###\n",
    "    src_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/\"\n",
    "    trg_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"\n",
    "    setupname = \"syn2syn\"\n",
    "    \n",
    "if setup==6: ### One-to-one Accent Addition from mix to syn ###\n",
    "    src_dir = []\n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"]\n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/\"]\n",
    "    trg_dir = [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"]*len(src_dir)\n",
    "    setupname = \"mix2syn\"\n",
    "    \n",
    "if setup==7: ### One-to-one Accent Addition Korean ###\n",
    "    src_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"\n",
    "    trg_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Korean/\"\n",
    "    setupname = \"gt2synKorean\"\n",
    "    \n",
    "if setup==8: ### One-to-one Accent Addition (Korean) from syn to syn ###\n",
    "    src_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/\"\n",
    "    trg_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Korean/\"\n",
    "    setupname = \"syn2synKorean\"\n",
    "    \n",
    "if setup==9: ### One-to-one Accent Addition (Korean) from mix to syn ###\n",
    "    src_dir = []\n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"]\n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/\"]\n",
    "    trg_dir = [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Korean/\"]*len(src_dir)\n",
    "    setupname = \"mix2synKorean\"\n",
    "    \n",
    "if setup==10: ### One-to-one Accent Addition, additional vctk\n",
    "    src_dir = []\n",
    "    trg_dir = []\n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"]\n",
    "    trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"]\n",
    "    \n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/\"]\n",
    "    trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"]\n",
    "    \n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT_add/English/\"]\n",
    "    trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT_add/Hindi/\"]\n",
    "    setupname = \"mix2synVCTK3hr\"\n",
    "    \n",
    "if setup==11: ### One-to-one Accent Removal, additional vctk\n",
    "    src_dir = []\n",
    "    trg_dir = []\n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"]\n",
    "    trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\"]\n",
    "    \n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/Hindi/\"]\n",
    "    trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/\"]\n",
    "    \n",
    "    src_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT_add/Hindi/\"]\n",
    "    trg_dir += [\"/mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT_add/English/\"]\n",
    "    setupname = \"syn2mixVCTK3hr\"\n",
    "\n",
    "scaler = {}\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R_80mel.save\"\n",
    "scaler[\"80mel\"] = joblib.load(scaler_filename)\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R_16000.save\"\n",
    "scaler[\"mel\"] = joblib.load(scaler_filename)\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R_wavlm.save\"\n",
    "scaler[\"wavlm\"] = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64ecf55-0e57-4891-9c20-68169f2224c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_fromhubert80mel_norepeating_smaller/checkpoint-100000steps.pkl\"\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_fromhubert80mel_norepeating_small/checkpoint-100000steps.pkl\"\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_fromhubert80mel_norepeating/checkpoint-100000steps.pkl\"\n",
    "\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_from1000hubert80mel_norepeating_small/checkpoint-100000steps.pkl\"\n",
    "# checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_from1000hubert80mel_norepeating/checkpoint-100000steps.pkl\"\n",
    "\n",
    "checkpoint_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/encoder_fine-tuning_nocondition_melmel_from300hubert80mel_norepeating/checkpoint-100000steps.pkl\"\n",
    "\n",
    "\n",
    "former_freeze = False # whether you only want to train the latter parts of encoder and decoder\n",
    "decoder_freeze = False\n",
    "guided_attn = False\n",
    "\n",
    "input_type = \"mel\"\n",
    "if \"smaller\" in checkpoint_path:\n",
    "    size = \"smaller\"\n",
    "elif \"small\" in checkpoint_path:\n",
    "    size = \"small\"\n",
    "else:\n",
    "    size = \"\"\n",
    "if \"addition\" in checkpoint_path:\n",
    "    conditiontype = \"add\"\n",
    "elif \"concatenation\" in checkpoint_path:\n",
    "    conditiontype = \"concat\"\n",
    "elif \"nocondition\" in checkpoint_path:\n",
    "    conditiontype = \"nocondition\"\n",
    "input_type = \"mel\"\n",
    "if \"80mel\" in checkpoint_path:\n",
    "    output_type = \"80mel\"\n",
    "    input_type = \"80mel\" if input_type==\"mel\" else input_type\n",
    "else:\n",
    "    output_type = \"mel\"\n",
    "inputoutput = [input_type, output_type] # mel to mimic wavlm\n",
    "\n",
    "args = {}\n",
    "args[\"rank\"] = 0\n",
    "sizename = \"_\"+size if size else \"\"\n",
    "# args[\"outdir\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/VTN_fine-tuning_{conditiontype}_{setupname}_{''.join(inputoutput)}_hubert_norepeating{sizename}_0.00001resume-from0.00008/\"\n",
    "# args[\"outdir\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/VTN_fine-tuning_{conditiontype}_{setupname}_{''.join(inputoutput)}_hubert_norepeating{sizename}/\"\n",
    "# args[\"outdir\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/VTN_fine-tuning_{conditiontype}_{setupname}_{''.join(inputoutput)}_1000hubert_norepeating{sizename}/\"\n",
    "args[\"outdir\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts_16000/VTN_fine-tuning_{conditiontype}_{setupname}_{''.join(inputoutput)}_300hubert_norepeating{sizename}/\"\n",
    "args[\"config_path\"] = f\"./../egs/l2-arctic/cascade/conf/{size}m2mvtn.melmel.yaml\"\n",
    "args[\"init_checkpoint\"] = checkpoint_path\n",
    "# args[\"resume\"] = args[\"outdir\"] + \"checkpoint-50000steps.pkl\"\n",
    "args[\"resume\"] = \"\"\n",
    "args[\"distributed\"] = False\n",
    "args = Dict2Obj(args)\n",
    "\n",
    "# load main config\n",
    "with open(args.config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "config.update(vars(args))\n",
    "\n",
    "# Customization\n",
    "config[\"batch_size\"] = 64\n",
    "config[\"model_params\"][\"conditiontype\"] = conditiontype\n",
    "config[\"optimizer_params\"][\"lr\"] = 0.00008\n",
    "config[\"use_guided_attn_loss\"] = guided_attn\n",
    "if decoder_freeze:\n",
    "    config[\"freeze-mods\"] = [\"decoder\", \"feat_out\", \"prob_out\", \"postnet\"]\n",
    "config[\"train_max_steps\"] = 100000\n",
    "if input_type==\"80mel\":\n",
    "    config[\"model_params\"][\"idim\"] = 80\n",
    "if output_type==\"80mel\":\n",
    "    config[\"model_params\"][\"odim\"] = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73d4fbe-2fe1-4791-832a-8cbbc6e37984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932 /mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\n",
      "932 /mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/\n",
      "4300 /mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT_add/English/\n",
      "100 /mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/SLT/\n",
      "100 /mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT/English/\n",
      "100 /mntcephfs/lab_data/shoinoue/Dataset/PD-AST/SLT_add/English/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Overriding module encoder.embed.conv.0.weight\n",
      "WARNING:root:Overriding module encoder.embed.conv.0.bias\n",
      "WARNING:root:Overriding module encoder.embed.conv.2.weight\n",
      "WARNING:root:Overriding module encoder.embed.conv.2.bias\n",
      "WARNING:root:Overriding module encoder.embed.out.0.weight\n",
      "WARNING:root:Overriding module encoder.embed.out.0.bias\n",
      "WARNING:root:Overriding module encoder.embed.out.1.alpha\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.norm2.bias\n",
      "WARNING:root:Overriding module encoder.after_norm.weight\n",
      "WARNING:root:Overriding module encoder.after_norm.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.0.0.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.0.0.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.1.0.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.1.0.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.1.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.1.bias\n",
      "WARNING:root:Overriding module decoder.embed.1.alpha\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm3.bias\n",
      "WARNING:root:Overriding module feat_out.weight\n",
      "WARNING:root:Overriding module feat_out.bias\n",
      "WARNING:root:Overriding module prob_out.weight\n",
      "WARNING:root:Overriding module prob_out.bias\n",
      "WARNING:root:Overriding module postnet.postnet.0.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.1.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.2.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.3.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.4.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.num_batches_tracked\n",
      "WARNING:root:Overriding module condition_encoding_embedding.weight\n",
      "WARNING:root:Overriding module condition_encoding_embedding.bias\n",
      "WARNING:root:Overriding module condition_decoding_embedding.weight\n",
      "WARNING:root:Overriding module condition_decoding_embedding.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_device(args.rank)\n",
    "if not os.path.exists(args.outdir):\n",
    "    os.makedirs(args.outdir)\n",
    "    \n",
    "### Dataset Preparation ###\n",
    "dataset = {\n",
    "    \"train\": ParallelArcticDataset(src_dir, trg_dir, datasplit, scaler, \"train\", input_output=inputoutput, noembedding=True),\n",
    "    \"dev\": ParallelArcticDataset(src_dir, trg_dir, datasplit, scaler, \"valid\", input_output=inputoutput, noembedding=True),\n",
    "}\n",
    "\n",
    "collater_class = getattr(\n",
    "    seq2seq_vc.collaters,\n",
    "    config.get(\"collater_type\", \"ARM2MVCCollater\"),\n",
    ")\n",
    "collater = collater_class()\n",
    "\n",
    "sampler = {\"train\": None, \"dev\": None}\n",
    "data_loader = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=dataset[\"train\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"train\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "    \"dev\": DataLoader(\n",
    "        dataset=dataset[\"dev\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"dev\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "}\n",
    "\n",
    "### Model Preparation ###\n",
    "model_class = getattr(\n",
    "    seq2seq_vc.models,\n",
    "    config.get(\"model_type\", \"M2MVTN\"),\n",
    ")\n",
    "model = model_class(**config[\"model_params\"]).to(device)\n",
    "\n",
    "if config.get(\"criterions\", None):\n",
    "    criterion = {\n",
    "        criterion_class: getattr(seq2seq_vc.losses, criterion_class)(\n",
    "            **criterion_paramaters\n",
    "        )\n",
    "        for criterion_class, criterion_paramaters in config[\"criterions\"].items()\n",
    "    }\n",
    "    if guided_attn:\n",
    "        criterion[\"guided_attn\"] = GuidedMultiHeadAttentionLoss()\n",
    "else:\n",
    "    raise ValueError(\"Please specify criterions in the config file.\")\n",
    "\n",
    "### optimizers and schedulers ###\n",
    "optimizer_class = getattr(\n",
    "    torch.optim,\n",
    "    # keep compatibility\n",
    "    config.get(\"optimizer_type\", \"Adam\"),\n",
    ")\n",
    "optimizer = optimizer_class(\n",
    "    model.parameters(),\n",
    "    **config[\"optimizer_params\"],\n",
    ")\n",
    "scheduler_class = scheduler_classes.get(config.get(\"scheduler_type\", \"warmuplr\"))\n",
    "scheduler = scheduler_class(\n",
    "    optimizer=optimizer,\n",
    "    **config[\"scheduler_params\"],\n",
    ")\n",
    "\n",
    "### define trainer ###\n",
    "trainer_class = getattr(\n",
    "    seq2seq_vc.trainers,\n",
    "    config.get(\"trainer_type\", \"ARM2MVCTrainer\"),\n",
    ")\n",
    "trainer = trainer_class(\n",
    "    steps=0,\n",
    "    epochs=0,\n",
    "    data_loader=data_loader,\n",
    "    sampler=sampler,\n",
    "    model=model,\n",
    "    vocoder=None,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# load pretrained parameters from checkpoint\n",
    "if len(args.init_checkpoint) != 0:\n",
    "    trainer.load_trained_modules(\n",
    "        args.init_checkpoint, init_mods=config[\"init-mods\"]\n",
    "    )\n",
    "\n",
    "# resume from checkpoint\n",
    "if len(args.resume) != 0:\n",
    "    trainer.load_checkpoint(args.resume)\n",
    "\n",
    "if former_freeze: # if you only want to train the latter parts of encoder and decoder\n",
    "    ehot = config[\"model_params\"][\"elayers\"]\n",
    "    ehot = np.arange(ehot)[-ehot//3:]\n",
    "    dhot = config[\"model_params\"][\"dlayers\"]\n",
    "    dhot = np.arange(dhot)[-dhot//3:]\n",
    "    dfixes = sorted(list(set([\"decoder.\"+\".\".join(a.split(\".\")[:2]) for a in list(model.decoder.state_dict().keys())])))\n",
    "    for i in dhot:\n",
    "        dfixes.remove(f\"decoder.decoders.{i}\")\n",
    "\n",
    "    efixes = sorted(list(set([\"encoder.\"+\".\".join(a.split(\".\")[:2]) for a in list(model.encoder.state_dict().keys())])))\n",
    "    for i in ehot:\n",
    "        efixes.remove(f\"encoder.encoders.{i}\")\n",
    "    config[\"freeze-mods\"] = dfixes + efixes\n",
    "    \n",
    "# freeze modules if necessary\n",
    "if config.get(\"freeze-mods\", None) is not None:\n",
    "    assert type(config[\"freeze-mods\"]) is list\n",
    "    trainer.freeze_modules(config[\"freeze-mods\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfdc6c57-cf10-42d2-a759-b9f28a9a1811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train]:   0%|          | 3/100000 [00:09<78:06:59,  2.81s/it] "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 31.74 GiB total capacity; 17.18 GiB already allocated; 14.88 MiB free; 17.49 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_checkpoint(\n\u001b[1;32m      5\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutdir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msteps.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:76\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtqdm \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m     72\u001b[0m     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_max_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[train]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# train one epoch\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# check whether training is finished\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinish_train:\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:134\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train model one epoch.\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_steps_per_epoch, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# train one step\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/ar_m2mvc.py:82\u001b[0m, in \u001b[0;36mARM2MVCTrainer._train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     71\u001b[0m yembs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myembs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# model forward\u001b[39;00m\n\u001b[1;32m     74\u001b[0m (\n\u001b[1;32m     75\u001b[0m     after_outs,\n\u001b[1;32m     76\u001b[0m     before_outs,\n\u001b[1;32m     77\u001b[0m     logits,\n\u001b[1;32m     78\u001b[0m     ys_,\n\u001b[1;32m     79\u001b[0m     labels_,\n\u001b[1;32m     80\u001b[0m     olens_,\n\u001b[1;32m     81\u001b[0m     (att_ws, ilens_ds_st, olens_in),\n\u001b[0;32m---> 82\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43milens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43molens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxembs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myembs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# seq2seq loss\u001b[39;00m\n\u001b[1;32m     85\u001b[0m l1_loss, bce_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeq2SeqLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[1;32m     86\u001b[0m     after_outs, before_outs, logits, ys_, labels_, olens_\n\u001b[1;32m     87\u001b[0m )\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/models/m2m_vtn.py:277\u001b[0m, in \u001b[0;36mM2MVTN.forward\u001b[0;34m(self, xs, ilens, ys, labels, olens, xembs, yembs, spembs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditiontype\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnocondition\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m zs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# zs = zs[:,:,:-1*yembs.shape[-1]] # Decouple\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# (B, Lmax//r, odim * r) -> (B, Lmax//r * r, odim)\u001b[39;00m\n\u001b[1;32m    281\u001b[0m before_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_out(zs)\u001b[38;5;241m.\u001b[39mview(zs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39modim)\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/modules/transformer/decoder.py:230\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, tgt, tgt_mask, memory, memory_mask)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward decoder.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(tgt)\n\u001b[0;32m--> 230\u001b[0m x, tgt_mask, memory, memory_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n\u001b[1;32m    234\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_norm(x)\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/modules/transformer/repeat.py:18\u001b[0m, in \u001b[0;36mMultiSequential.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Repeat.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/modules/transformer/decoder_layer.py:107\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, tgt, tgt_mask, memory, memory_mask, cache)\u001b[0m\n\u001b[1;32m    105\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_linear1(tgt_concat)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_q_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/modules/transformer/attention.py:110\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute scaled dot product attention.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_qkv(query, key, value)\n\u001b[0;32m--> 110\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_attention(v, scores, mask)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 31.74 GiB total capacity; 17.18 GiB already allocated; 14.88 MiB free; 17.49 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.run()\n",
    "finally:\n",
    "    trainer.save_checkpoint(\n",
    "        os.path.join(config[\"outdir\"], f\"checkpoint-{trainer.steps}steps.pkl\")\n",
    "    )\n",
    "    logging.info(f\"Successfully saved checkpoint @ {trainer.steps}steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2656dd1-344c-4648-a085-ee5c3dc51c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2f054-78a0-4e78-85b4-1c8c6638a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyfiles.feature_extractor import get_vocos\n",
    "# ### Vocoder Preparation ###\n",
    "# data_dir = \"/mntcephfs/lab_data/shoinoue/\"\n",
    "# # fs = 24000\n",
    "# fs = 16000\n",
    "\n",
    "# if fs==24000:\n",
    "#     config_path = f\"{data_dir}Models/trained_models/vocos/24k/config.yaml\"\n",
    "#     model_path = f\"{data_dir}Models/trained_models/vocos/24k/pytorch_model.bin\"\n",
    "# elif fs==16000:\n",
    "#     config_path = f\"{data_dir}Models/trained_models/vocos/vocos16k_noncausal_tealab/config16k.yaml\"\n",
    "#     model_path = f\"{data_dir}Models/trained_models/vocos/vocos16k_noncausal_tealab/vocos16k_noncausal_last.ckpt\"\n",
    "# vocoder = get_vocos(config_path, model_path, fs)\n",
    "\n",
    "# mel = dataset[0][\"src_feat\"]\n",
    "# mel = scaler[\"mel\"].inverse_transform(mel)\n",
    "# y = vocoder.decode(torch.tensor(mel.T).unsqueeze(0)).cpu().numpy()\n",
    "# play_audio(y, fs)\n",
    "\n",
    "# mel = dataset[0][\"trg_feat\"]\n",
    "# mel = scaler[\"mel\"].inverse_transform(mel)\n",
    "# y = vocoder.decode(torch.tensor(mel.T).unsqueeze(0)).cpu().numpy()\n",
    "# play_audio(y, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009bcab-2131-4062-a204-2bde9bf646b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
