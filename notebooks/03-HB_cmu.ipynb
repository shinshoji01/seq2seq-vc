{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24182e13-4f90-4085-b10d-80dd9894bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import librosa\n",
    "import glob\n",
    "\n",
    "sys.path.append(\"../../cuhksz-phd/sho_util/pyfiles/\")\n",
    "from pytorch import cuda2numpy, cuda2cpu\n",
    "from basic import plot_spectrogram\n",
    "from sound import play_audio\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from pyfiles.processsound import trim_audio_and_save\n",
    "\n",
    "sys.path.append(\"./../../BigVGAN/\")\n",
    "import bigvgan\n",
    "from meldataset import get_mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252570ef-9653-4f4c-ace2-b41603d3c118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config.json from local directory\n",
      "Loading weights from local directory\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/mntcephfs/lab_data/shoinoue/\"\n",
    "dataset_dir = data_dir + \"Dataset/CMU-ARCTIC/\"\n",
    "fs = 16000\n",
    "trim_threshold_in_db = 30\n",
    "speakers = [\"SLT\"]\n",
    "\n",
    "# modelpath = \"/mntcephfs/data/audiow/shoinoue/Model/hf_hub/bigvgan/models--nvidia--bigvgan_v2_24khz_100band_256x/snapshots/61df17db326f0876b7201d7a56c831898c836ef4\"\n",
    "modelpath = \"/mntcephfs/data/audiow/shoinoue/Model/hf_hub/bigvgan/models--nvidia--bigvgan_v2_22khz_80band_fmax8k_256x/snapshots/189a02ed3b7957e8534b40e6314262df53536ece\"\n",
    "model = bigvgan.BigVGAN.from_pretrained(modelpath, use_cuda_kernel=False)\n",
    "model.remove_weight_norm()\n",
    "model = model.eval()\n",
    "\n",
    "def get_mel(path):\n",
    "    trim_audio_and_save(path, savepath=tempfile, trim_threshold_in_db=trim_threshold_in_db)\n",
    "    wav, sr = librosa.load(tempfile, sr=model.h.sampling_rate, mono=True) \n",
    "    wav = torch.FloatTensor(wav).unsqueeze(0)\n",
    "    mel = get_mel_spectrogram(wav, model.h)\n",
    "    mel = np.array(mel[0])\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f9a7a89-b74f-4c04-9167-c5f7f422775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1132/1132 [01:40<00:00, 11.30it/s]\n"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "tempfile = \"temp2.wav\"\n",
    "\n",
    "for spk in speakers:\n",
    "    print(spk)\n",
    "    mel_dir = f\"{dataset_dir}{spk}/80mel/\"\n",
    "    os.makedirs(mel_dir, exist_ok=True)\n",
    "    filenames = glob.glob(dataset_dir + spk + \"/wav/*\")\n",
    "    filenames.sort()\n",
    "    for path in tqdm(filenames):\n",
    "        trim_audio_and_save(path, savepath=tempfile, trim_threshold_in_db=trim_threshold_in_db)\n",
    "            \n",
    "        # mel\n",
    "        mel = get_mel(tempfile)\n",
    "        savepath = mel_dir + os.path.basename(path)[:-4] + \".npy\"\n",
    "        if save:\n",
    "            np.save(savepath, mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9931a4f2-a69e-4d36-83aa-d15aefd79dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
