{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ae1fd-ea23-4724-b100-8c789924c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pyfiles.dataset import Parallelo2oVCMelDataset\n",
    "\n",
    "import seq2seq_vc\n",
    "import seq2seq_vc.models\n",
    "import seq2seq_vc.losses\n",
    "import seq2seq_vc.trainers\n",
    "import seq2seq_vc.collaters\n",
    "\n",
    "# from seq2seq_vc.datasets import ParallelVCMelDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from seq2seq_vc.utils import read_hdf5\n",
    "from seq2seq_vc.utils.types import str_or_none\n",
    "\n",
    "# set to avoid matplotlib error in CLI environment\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seq2seq_vc.schedulers.warmup_lr import WarmupLR\n",
    "\n",
    "scheduler_classes = dict(warmuplr=WarmupLR)\n",
    "\n",
    "class Dict2Obj(object):\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])\n",
    "\n",
    "import joblib\n",
    "import glob\n",
    "datasplit = list(np.load(\"./data_split_ARCTIC.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70a307d-1a6a-4fb4-92d8-10812763aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Variables\n",
    "dataset_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/L2-ARCTIC/\"\n",
    "speakers = [\"HKK\", \"TNI\"]\n",
    "src_spk = \"HKK\"\n",
    "trg_spk = \"TNI\"\n",
    "\n",
    "# scaler_filename = f\"ckpts/scalers/HKK_TNI.save\"\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R.save\"\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a64ecf55-0e57-4891-9c20-68169f2224c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"rank\"] = 0\n",
    "args[\"outdir\"] = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts/fine-tuning_concatenation_o2o_HKK-TNI/\"\n",
    "args[\"config_path\"] = \"./../egs/l2-arctic/cascade/conf/m2mvtn.melmel.yaml\"\n",
    "args[\"init_checkpoint\"] = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts/pretraining_concatenation_LibriTTS-R/checkpoint-200000steps.pkl\"\n",
    "args[\"resume\"] = \"\"\n",
    "args[\"distributed\"] = False\n",
    "args = Dict2Obj(args)\n",
    "\n",
    "# load main config\n",
    "with open(args.config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "config.update(vars(args))\n",
    "\n",
    "# Customization\n",
    "config[\"batch_size\"] = 32\n",
    "config[\"model_params\"][\"conditiontype\"] = \"add\" if \"add\" in args.init_checkpoint else \"concat\"\n",
    "config[\"randomcondition\"] = \"randomcondition\" in args.outdir\n",
    "config[\"optimizer_params\"][\"lr\"] = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73d4fbe-2fe1-4791-832a-8cbbc6e37984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Overriding module encoder.embed.conv.0.weight\n",
      "WARNING:root:Overriding module encoder.embed.conv.0.bias\n",
      "WARNING:root:Overriding module encoder.embed.conv.2.weight\n",
      "WARNING:root:Overriding module encoder.embed.conv.2.bias\n",
      "WARNING:root:Overriding module encoder.embed.out.0.weight\n",
      "WARNING:root:Overriding module encoder.embed.out.0.bias\n",
      "WARNING:root:Overriding module encoder.embed.out.1.alpha\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.0.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.1.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.2.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.3.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.4.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.5.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.6.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.6.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.7.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.7.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.8.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.8.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.9.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.9.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.10.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.10.norm2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.norm1.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.norm1.bias\n",
      "WARNING:root:Overriding module encoder.encoders.11.norm2.weight\n",
      "WARNING:root:Overriding module encoder.encoders.11.norm2.bias\n",
      "WARNING:root:Overriding module encoder.after_norm.weight\n",
      "WARNING:root:Overriding module encoder.after_norm.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.0.0.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.0.0.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.1.0.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.0.prenet.1.0.bias\n",
      "WARNING:root:Overriding module decoder.embed.0.1.weight\n",
      "WARNING:root:Overriding module decoder.embed.0.1.bias\n",
      "WARNING:root:Overriding module decoder.embed.1.alpha\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.0.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.1.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.2.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.3.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.4.norm3.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.self_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_q.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_q.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_k.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_k.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_v.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_v.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_out.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.src_attn.linear_out.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.feed_forward.w_2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm1.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm1.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm2.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm2.bias\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm3.weight\n",
      "WARNING:root:Overriding module decoder.decoders.5.norm3.bias\n",
      "WARNING:root:Overriding module feat_out.weight\n",
      "WARNING:root:Overriding module feat_out.bias\n",
      "WARNING:root:Overriding module prob_out.weight\n",
      "WARNING:root:Overriding module prob_out.bias\n",
      "WARNING:root:Overriding module postnet.postnet.0.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.0.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.1.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.1.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.2.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.2.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.3.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.3.1.num_batches_tracked\n",
      "WARNING:root:Overriding module postnet.postnet.4.0.weight\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.weight\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.bias\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.running_mean\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.running_var\n",
      "WARNING:root:Overriding module postnet.postnet.4.1.num_batches_tracked\n",
      "WARNING:root:Overriding module condition_encoding_embedding.weight\n",
      "WARNING:root:Overriding module condition_encoding_embedding.bias\n",
      "WARNING:root:Overriding module condition_decoding_embedding.weight\n",
      "WARNING:root:Overriding module condition_decoding_embedding.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_device(args.rank)\n",
    "if not os.path.exists(args.outdir):\n",
    "    os.makedirs(args.outdir)\n",
    "    \n",
    "### Dataset Preparation ###\n",
    "dataset = {\n",
    "    \"train\": Parallelo2oVCMelDataset(dataset_dir, speakers, src_spk, trg_spk, datasplit, scaler, \"train\", randomcondition=config[\"randomcondition\"]),\n",
    "    \"dev\": Parallelo2oVCMelDataset(dataset_dir, speakers, src_spk, trg_spk, datasplit, scaler, \"valid\", randomcondition=config[\"randomcondition\"]),\n",
    "}\n",
    "\n",
    "collater_class = getattr(\n",
    "    seq2seq_vc.collaters,\n",
    "    config.get(\"collater_type\", \"ARM2MVCCollater\"),\n",
    ")\n",
    "collater = collater_class()\n",
    "\n",
    "sampler = {\"train\": None, \"dev\": None}\n",
    "data_loader = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=dataset[\"train\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"train\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "    \"dev\": DataLoader(\n",
    "        dataset=dataset[\"dev\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"dev\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "}\n",
    "\n",
    "### Model Preparation ###\n",
    "model_class = getattr(\n",
    "    seq2seq_vc.models,\n",
    "    config.get(\"model_type\", \"M2MVTN\"),\n",
    ")\n",
    "model = model_class(**config[\"model_params\"]).to(device)\n",
    "\n",
    "if config.get(\"criterions\", None):\n",
    "    criterion = {\n",
    "        criterion_class: getattr(seq2seq_vc.losses, criterion_class)(\n",
    "            **criterion_paramaters\n",
    "        )\n",
    "        for criterion_class, criterion_paramaters in config[\"criterions\"].items()\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"Please specify criterions in the config file.\")\n",
    "\n",
    "### optimizers and schedulers ###\n",
    "optimizer_class = getattr(\n",
    "    torch.optim,\n",
    "    # keep compatibility\n",
    "    config.get(\"optimizer_type\", \"Adam\"),\n",
    ")\n",
    "optimizer = optimizer_class(\n",
    "    model.parameters(),\n",
    "    **config[\"optimizer_params\"],\n",
    ")\n",
    "scheduler_class = scheduler_classes.get(config.get(\"scheduler_type\", \"warmuplr\"))\n",
    "scheduler = scheduler_class(\n",
    "    optimizer=optimizer,\n",
    "    **config[\"scheduler_params\"],\n",
    ")\n",
    "\n",
    "### define trainer ###\n",
    "trainer_class = getattr(\n",
    "    seq2seq_vc.trainers,\n",
    "    config.get(\"trainer_type\", \"ARM2MVCTrainer\"),\n",
    ")\n",
    "trainer = trainer_class(\n",
    "    steps=0,\n",
    "    epochs=0,\n",
    "    data_loader=data_loader,\n",
    "    sampler=sampler,\n",
    "    model=model,\n",
    "    vocoder=None,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# load pretrained parameters from checkpoint\n",
    "if len(args.init_checkpoint) != 0:\n",
    "    trainer.load_trained_modules(\n",
    "        args.init_checkpoint, init_mods=config[\"init-mods\"]\n",
    "    )\n",
    "\n",
    "# resume from checkpoint\n",
    "if len(args.resume) != 0:\n",
    "    trainer.load_checkpoint(args.resume)\n",
    "\n",
    "# freeze modules if necessary\n",
    "if config.get(\"freeze-mods\", None) is not None:\n",
    "    assert type(config[\"freeze-mods\"]) is list\n",
    "    trainer.freeze_modules(config[\"freeze-mods\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdc6c57-cf10-42d2-a759-b9f28a9a1811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train]:   0%|          | 2/100000 [00:08<118:11:13,  4.25s/it]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.59 GiB (GPU 0; 31.74 GiB total capacity; 9.22 GiB already allocated; 1.12 GiB free; 16.64 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_checkpoint(\n\u001b[1;32m      5\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutdir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msteps.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     )\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:76\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtqdm \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m     72\u001b[0m     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_max_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[train]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# train one epoch\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# check whether training is finished\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinish_train:\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:134\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train model one epoch.\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_steps_per_epoch, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# train one step\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/ar_m2mvc.py:102\u001b[0m, in \u001b[0;36mARM2MVCTrainer._train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# update model\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 102\u001b[0m \u001b[43mgen_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    107\u001b[0m     )\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.59 GiB (GPU 0; 31.74 GiB total capacity; 9.22 GiB already allocated; 1.12 GiB free; 16.64 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.run()\n",
    "finally:\n",
    "    trainer.save_checkpoint(\n",
    "        os.path.join(config[\"outdir\"], f\"checkpoint-{trainer.steps}steps.pkl\")\n",
    "    )\n",
    "    logging.info(f\"Successfully saved checkpoint @ {trainer.steps}steps.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
