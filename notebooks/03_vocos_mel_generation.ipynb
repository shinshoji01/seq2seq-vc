{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24182e13-4f90-4085-b10d-80dd9894bb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import tgt\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import librosa\n",
    "import glob\n",
    "\n",
    "sys.path.append(\"/mntcephfs/lab_data/shoinoue/Models/trained_models/vocos/vocos16k_noncausal_tealab/\")\n",
    "from vocos16k_inference import Vocos\n",
    "\n",
    "sys.path.append(\"../../cuhksz-phd/sho_util/pyfiles/\")\n",
    "from pytorch import cuda2numpy, cuda2cpu\n",
    "from basic import plot_spectrogram\n",
    "from sound import play_audio\n",
    "\n",
    "from vocos.feature_extractors import MelSpectrogramFeatures, EncodecFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "252570ef-9653-4f4c-ace2-b41603d3c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mntcephfs/lab_data/shoinoue/\"\n",
    "dataset_dir = data_dir + \"Dataset/L2-ARCTIC/\"\n",
    "fs = 24000\n",
    "\n",
    "melgen = MelSpectrogramFeatures(sample_rate=fs)\n",
    "speakers = [os.path.basename(a) for a in glob.glob(dataset_dir + \"*\")]\n",
    "speakers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16af6b55-0580-4793-914f-0d9c3c5e8c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HKK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:13<00:00, 82.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:16<00:00, 68.38it/s]\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "\n",
    "for spk in speakers:\n",
    "    print(spk)\n",
    "    mel_dir = f\"{dataset_dir}{spk}/mel/\"\n",
    "    os.makedirs(mel_dir, exist_ok=True)\n",
    "    filenames = glob.glob(dataset_dir + spk + \"/wav/*\")\n",
    "    filenames.sort()\n",
    "    for path in tqdm(filenames):\n",
    "        y, sr = torchaudio.load(path)\n",
    "        if fs!=sr:\n",
    "            if y.size(0) > 1:  # mix to mono\n",
    "                y = y.mean(dim=0, keepdim=True)\n",
    "            y = torchaudio.functional.resample(y, orig_freq=sr, new_freq=fs)\n",
    "        mel = melgen(y[0])\n",
    "        savepath = mel_dir + os.path.basename(path)[:-4] + \".npy\"\n",
    "        if save:\n",
    "            np.save(savepath, mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd08a4-73b9-4775-a1ab-a0e4623fbe78",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d78731b-6f00-4ce1-8c4a-84bdb4f1a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "datasplit = list(np.load(\"./data_split_ARCTIC.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2a5479-2d35-42e1-b992-2c1d8684bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_speakers = [\"HKK\", \"TNI\"] \n",
    "\n",
    "if target_speakers==[\"HKK\", \"TNI\"]:\n",
    "    scaler_name = \"HKK_TNI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a73d1afb-5a45-4c87-a2e5-c43afa2b4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HKK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 1592.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 1682.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ckpts/scalers/HKK_TNI.save']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "for spk in target_speakers:\n",
    "    print(spk)\n",
    "    mel_dir = f\"{dataset_dir}{spk}/mel/\"\n",
    "    files = glob.glob(mel_dir + \"*\")\n",
    "    files.sort()\n",
    "    for path in tqdm(files):\n",
    "        if os.path.basename(path)[:-4] in datasplit[0]: # sample must be in train\n",
    "            mel = np.load(path)\n",
    "            scaler.partial_fit(mel.T)\n",
    "            \n",
    "scaler_filename = f\"ckpts/scalers/{scaler_name}.save\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "# a = joblib.load(scaler_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
