{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82852c00-47cd-4309-818e-8905731ebe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 20:05:56,689 - modelscope - INFO - PyTorch version 2.0.1+cu118 Found.\n",
      "2024-07-31 20:05:56,693 - modelscope - INFO - Loading ast index from /home/shoinoue/.cache/modelscope/ast_indexer\n",
      "2024-07-31 20:05:56,838 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 b628653653bc205329c92249435e1927 and a total number of 980 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer is not installed, please install it if you want to use related modules\n",
      "failed to import ttsfrd, use WeTextProcessing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 20:06:06.915018664 [E:onnxruntime:Default, provider_bridge_ort.cc:1480 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1193 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "\n",
      "2024-07-31 20:06:06.915060767 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:743 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "2024-07-31 20:06:07,765 WETEXT INFO found existing fst: /mntcephfs/lab_data/shoinoue/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\n",
      "INFO:wetext-zh_normalizer:found existing fst: /mntcephfs/lab_data/shoinoue/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_tagger.fst\n",
      "2024-07-31 20:06:07,769 WETEXT INFO                     /mntcephfs/lab_data/shoinoue/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\n",
      "INFO:wetext-zh_normalizer:                    /mntcephfs/lab_data/shoinoue/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2024-07-31 20:06:07,772 WETEXT INFO skip building fst for zh_normalizer ...\n",
      "INFO:wetext-zh_normalizer:skip building fst for zh_normalizer ...\n",
      "2024-07-31 20:06:08,373 WETEXT INFO found existing fst: /mntcephfs/lab_data/shoinoue/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_tagger.fst\n",
      "INFO:wetext-en_normalizer:found existing fst: /mntcephfs/lab_data/shoinoue/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_tagger.fst\n",
      "2024-07-31 20:06:08,376 WETEXT INFO                     /mntcephfs/lab_data/shoinoue/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\n",
      "INFO:wetext-en_normalizer:                    /mntcephfs/lab_data/shoinoue/miniconda3/envs/cosyvoice/lib/python3.8/site-packages/tn/en_tn_verbalizer.fst\n",
      "2024-07-31 20:06:08,379 WETEXT INFO skip building fst for en_normalizer ...\n",
      "INFO:wetext-en_normalizer:skip building fst for en_normalizer ...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/shoinoue/Git/CosyVoice/\")\n",
    "sys.path.append(\"/home/shoinoue/Git/CosyVoice/third_party/Matcha-TTS/\")\n",
    "\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice\n",
    "from cosyvoice.utils.file_utils import load_wav\n",
    "# import torchaudio\n",
    "import torch\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "def play_audio(data, rate):\n",
    "    IPython.display.display(IPython.display.Audio(data=data,rate=rate))\n",
    "    \n",
    "cosyvoice = CosyVoice('/mntcephfs/data/audiow/shoinoue/Model/models/cosyvoice/CosyVoice-300M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec69cdb-9c9f-41df-b981-ef36ca85ed49",
   "metadata": {},
   "source": [
    "- Cross Lingual for L2-ARCTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7ffff9-3dba-4dcf-9150-3895bab3f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # speakers = [\"ASI\", \"RRBI\", \"SVBI\", \"TNI\", \"ABA\", \"SKA\", \"YBAA\", \"ZHAA\"]\n",
    "# speakers = [\"ASI\", \"RRBI\", \"SVBI\", \"TNI\"]\n",
    "# sample_num = 100000000\n",
    "# repeat_num = 2\n",
    "# save = False\n",
    "# play = False\n",
    "\n",
    "# for spk in speakers:\n",
    "#     print(spk)\n",
    "#     base_dir = f\"/mntcephfs/lab_data/shoinoue/Dataset/L2-ARCTIC/{spk}/wav/\"\n",
    "#     text_dir = \"./SPAT/transliteration/\"\n",
    "#     save_dir = f\"/mntcephfs/data/audiow/shoinoue/Dataset/CosyVoice/{spk}_allsamples/English/wav/\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     audiofiles = glob.glob(base_dir + \"*.wav\")\n",
    "#     audiofiles.sort()\n",
    "    \n",
    "#     for path in tqdm(audiofiles[:sample_num]):\n",
    "#         prompt_speech_16k = load_wav(path, 16000)\n",
    "#         basename = os.path.basename(path)[:-4]\n",
    "#         textpath = text_dir + basename + \".npy\"\n",
    "#         sentences = np.load(textpath, allow_pickle=True).item()\n",
    "#         for r in range(repeat_num):\n",
    "#             output = cosyvoice.inference_cross_lingual(f'<|en|>{sentences[\"English\"]}', prompt_speech_16k)\n",
    "#             savepath = save_dir + basename + f\"-{r}.wav\"\n",
    "#             if save:\n",
    "#                 # torchaudio.save(savepath, output['tts_speech'], 22050)\n",
    "#                 write_wav(savepath, 22050, output[\"tts_speech\"][0].numpy())\n",
    "#             if play:\n",
    "#                 x = load_wav(savepath, 16000)\n",
    "#                 play_audio(prompt_speech_16k, 16000)\n",
    "#                 play_audio(x, 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28fe5d-2720-4022-aeaf-968ba3ba95dc",
   "metadata": {},
   "source": [
    "- Speaker Accent Different Speech Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b15610-561f-4fed-b612-27398a2481c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosyvoice_zeroshot(tts_text, prompt_text, xv_speech_16k, prompt_speech_16k):\n",
    "    # prompt_speech_16k: speech prompt for accent\n",
    "    # xv_speech_16k: speech prompt for speaker (xvector)\n",
    "    \n",
    "    prompt_text = cosyvoice.frontend.text_normalize(prompt_text, split=False)\n",
    "    tts_speeches = []\n",
    "    for i in cosyvoice.frontend.text_normalize(tts_text, split=True):\n",
    "        model_input = cosyvoice.frontend.frontend_zero_shot(i, prompt_text, xv_speech_16k)\n",
    "        xv = cosyvoice.frontend.frontend_zero_shot(i, prompt_text, prompt_speech_16k)\n",
    "        for key in ['llm_prompt_speech_token', 'llm_prompt_speech_token_len', 'llm_embedding']: # llm is speech prompt\n",
    "        # for key in ['llm_embedding']:\n",
    "            model_input[key] = xv[key]\n",
    "        model_output = cosyvoice.model.inference(**model_input)\n",
    "        tts_speeches.append(model_output['tts_speech'])\n",
    "    output =  {'tts_speech': torch.concat(tts_speeches, dim=1)}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642da576-67f0-4933-b4be-a7ced038bf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 312/1132 [13:43<36:03,  2.64s/it]  \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# speakers = [\"SLT\", \"BDL\", \"EEY\", \"RMS\", \"AEW\", \"CLB\", \"LJM\", \"LNH\"]\n",
    "speakers = [\"SLT\", \"BDL\", \"EEY\", \"RMS\"]\n",
    "text_dir = \"./SPAT/transliteration/\"\n",
    "repeat_num = 3\n",
    "\n",
    "save = True\n",
    "play = False\n",
    "for spk in speakers:\n",
    "    base_dir = f\"/mntcephfs/lab_data/shoinoue/Dataset/CMU-ARCTIC/{spk}/wav/\"\n",
    "    prompt_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/L2-ARCTIC/ASI/wav/\"\n",
    "    save_dir = f\"/mntcephfs/data/audiow/shoinoue/Dataset/CosyVoice/{spk}_allsamples/ASI/wav/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    audiofiles = glob.glob(base_dir + \"*.wav\")\n",
    "    audiofiles.sort()\n",
    "    print(spk)\n",
    "    for p in tqdm(range(len(audiofiles))):\n",
    "        path = audiofiles[p]\n",
    "        basename = os.path.basename(path)[:-4]\n",
    "        start = len(glob.glob(save_dir + basename + f\"-*.wav\"))\n",
    "        if start>=repeat_num:\n",
    "            continue\n",
    "        xv_speech_16k = load_wav(path, 16000)\n",
    "        textpath = text_dir + basename + \".npy\"\n",
    "        sentences = np.load(textpath, allow_pickle=True).item()\n",
    "\n",
    "        # promptpath = prompt_dir + os.path.basename(audiofiles[p+1])[:-4] + \".wav\"\n",
    "        promptpath = prompt_dir + os.path.basename(audiofiles[p])[:-4] + \".wav\"\n",
    "        if not(os.path.exists(promptpath)):\n",
    "            promptpath = glob.glob(prompt_dir + \"*.wav\")\n",
    "            promptpath = promptpath[np.random.randint(len(promptpath))]\n",
    "        prompt_speech_16k = load_wav(promptpath, 16000)\n",
    "        prompttextpath = text_dir + os.path.basename(promptpath)[:-4] + \".npy\"\n",
    "        promptsentences = np.load(prompttextpath, allow_pickle=True).item()\n",
    "\n",
    "        tts_text = f'<|en|>{sentences[\"English\"]}'\n",
    "        prompt_text = f'<|en|>{promptsentences[\"English\"]}'\n",
    "        # tts_text = f'{sentences[\"English\"]}'\n",
    "        # prompt_text = f'{promptsentences[\"English\"]}'\n",
    "        \n",
    "        for r in range(start, repeat_num):\n",
    "            output = cosyvoice_zeroshot(tts_text, prompt_text, xv_speech_16k, prompt_speech_16k)\n",
    "            savepath = save_dir + basename + f\"-{r}.wav\"\n",
    "\n",
    "            if save:\n",
    "                write_wav(savepath, 22050, output[\"tts_speech\"][0].numpy())\n",
    "            if play:\n",
    "                print(\"accent prompt\")\n",
    "                play_audio(prompt_speech_16k, 16000)\n",
    "                print(\"speaker prompt\")\n",
    "                play_audio(xv_speech_16k, 16000)\n",
    "                if save:\n",
    "                    print(\"synthesized\")\n",
    "                    x = load_wav(savepath, 16000)\n",
    "                    play_audio(x, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b144a-3a63-48f9-b182-156aedad8bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
