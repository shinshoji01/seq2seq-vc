{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c14ae1fd-ea23-4724-b100-8c789924c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../cuhksz-phd/sho_util/pyfiles/\")\n",
    "from basic import plot_spectrogram\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from pyfiles.dataset import PretrainingMelDataset\n",
    "from pyfiles.utils import Dict2Obj\n",
    "\n",
    "import seq2seq_vc\n",
    "import seq2seq_vc.models\n",
    "import seq2seq_vc.losses\n",
    "import seq2seq_vc.trainers\n",
    "import seq2seq_vc.collaters\n",
    "\n",
    "# from seq2seq_vc.datasets import ParallelVCMelDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from seq2seq_vc.utils import read_hdf5\n",
    "from seq2seq_vc.utils.types import str_or_none\n",
    "# from seq2seq_vc.vocoder import Vocoder\n",
    "# from seq2seq_vc.vocoder.s3prl_feat2wav import S3PRL_Feat2Wav\n",
    "# from seq2seq_vc.vocoder.griffin_lim import Spectrogram2Waveform\n",
    "# from seq2seq_vc.vocoder.encodec import EnCodec_decoder\n",
    "\n",
    "# set to avoid matplotlib error in CLI environment\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seq2seq_vc.schedulers.warmup_lr import WarmupLR\n",
    "\n",
    "scheduler_classes = dict(warmuplr=WarmupLR)\n",
    "\n",
    "class Dict2Obj(object):\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])\n",
    "\n",
    "import joblib\n",
    "import glob\n",
    "datasplit = list(np.load(\"./data_split_ARCTIC.npy\", allow_pickle=True))\n",
    "information = [\n",
    "    [\"ABA\", \"Arabic\", \"M\"],\n",
    "    [\"SKA\", \"Arabic\", \"F\"],\n",
    "    [\"YBAA\", \"Arabic\", \"M\"],\n",
    "    [\"ZHAA\", \"Arabic\", \"F\"],\n",
    "    [\"BWC\", \"Mandarin\", \"M\"],\n",
    "    [\"LXC\", \"Mandarin\", \"F\"],\n",
    "    [\"NCC\", \"Mandarin\", \"F\"],\n",
    "    [\"TXHC\", \"Mandarin\", \"M\"],\n",
    "    [\"ASI\", \"Hindi\", \"M\"],\n",
    "    [\"RRBI\", \"Hindi\", \"M\"],\n",
    "    [\"SVBI\", \"Hindi\", \"F\"],\n",
    "    [\"TNI\", \"Hindi\", \"F\"],\n",
    "    [\"HJK\", \"Korean\", \"F\"],\n",
    "    [\"HKK\", \"Korean\", \"M\"],\n",
    "    [\"YDCK\", \"Korean\", \"F\"],\n",
    "    [\"YKWK\", \"Korean\", \"M\"],\n",
    "    [\"EBVS\", \"Spanish\", \"M\"],\n",
    "    [\"ERMS\", \"Spanish\", \"M\"],\n",
    "    [\"MBMPS\", \"Spanish\", \"F\"],\n",
    "    [\"NJS\", \"Spanish\", \"F\"],\n",
    "    [\"HQTV\", \"Vietnamese\", \"M\"],\n",
    "    [\"PNV\", \"Vietnamese\", \"F\"],\n",
    "    [\"THV\", \"Vietnamese\", \"F\"],\n",
    "    [\"TLV\", \"Vietnamese\", \"M\"],\n",
    "]\n",
    "spk2acc = {info[0]: info[1] for info in information}\n",
    "spk2sex = {info[0]: info[2] for info in information}\n",
    "acc2spk = {key: [] for key in set(list(spk2acc.values()))}\n",
    "sex2spk = {key: [] for key in set(list(spk2sex.values()))}\n",
    "for spk in spk2acc:\n",
    "    acc2spk[spk2acc[spk]] += [spk]\n",
    "    sex2spk[spk2sex[spk]] += [spk]\n",
    "accents = list(acc2spk.keys())\n",
    "accents.sort()\n",
    "speakers = list(spk2acc.keys())\n",
    "speakers.sort()\n",
    "genders = list(sex2spk.keys())\n",
    "genders.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1529d37-ef5f-4789-ba36-fe85beb0390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainingL2Arctic(Dataset):\n",
    "    def __init__(self, dataset_dir, speakers, accents, genders, datasplit, scaler, mode=\"train\", input_output_type=[\"wavlm\", \"mel\"]):\n",
    "        modefiles = datasplit[[\"train\", \"valid\", \"test\"].index(mode)]\n",
    "        data = {}\n",
    "        for spk in speakers:\n",
    "            data[spk] = []\n",
    "            for a in glob.glob(dataset_dir + f\"{spk}/{input_output_type[0]}/*.npy\"):\n",
    "                basename = os.path.basename(a)[:-4]\n",
    "                if basename in modefiles:\n",
    "                    data[spk] += [basename]\n",
    "            data[spk].sort()\n",
    "                \n",
    "        files = []\n",
    "        for s, spk in enumerate(speakers):\n",
    "            accentid = accents.index(spk2acc[spk])\n",
    "            genderid = genders.index(spk2sex[spk])\n",
    "            for basename in data[spk]:\n",
    "                files += [[s, accentid, genderid, basename]]\n",
    "            \n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.data = data\n",
    "        self.scaler = scaler\n",
    "        self.files = files\n",
    "        self.speakers = speakers\n",
    "        self.accents = accents\n",
    "        self.genders = genders\n",
    "        self.input_output_type = input_output_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        speaker, accent, gender, basename = self.files[idx]\n",
    "        items = {}\n",
    "        src_mel = self.dataset_dir + f\"{self.speakers[speaker]}/{self.input_output_type[0]}/{basename}.npy\"\n",
    "        trg_mel = self.dataset_dir + f\"{self.speakers[speaker]}/{self.input_output_type[1]}/{basename}.npy\"\n",
    "        items[\"src_feat\"] = self.scaler[self.input_output_type[0]].transform(np.load(src_mel).T)\n",
    "        items[\"trg_feat\"] = self.scaler[self.input_output_type[1]].transform(np.load(trg_mel).T)\n",
    "        items[\"src_condition\"] = np.load(src_mel.replace(self.input_output_type[0], \"accent_embedding\"))\n",
    "        items[\"trg_condition\"] = np.load(trg_mel.replace(self.input_output_type[1], \"accent_embedding\"))\n",
    "        items[\"utt_id\"] = basename\n",
    "        items[\"speaker_id\"] = speaker\n",
    "        items[\"accent_id\"] = accent\n",
    "        items[\"gender_id\"] = gender\n",
    "        \n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70a307d-1a6a-4fb4-92d8-10812763aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Variables\n",
    "dataset_dir = \"/mntcephfs/lab_data/shoinoue/Dataset/L2-ARCTIC/\"\n",
    "\n",
    "scaler = {}\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R.save\"\n",
    "scaler[\"mel\"] = joblib.load(scaler_filename)\n",
    "scaler_filename = f\"ckpts/scalers/LibriTTS-R_wavlm.save\"\n",
    "scaler[\"wavlm\"] = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64ecf55-0e57-4891-9c20-68169f2224c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditiontype = \"nocondition\"\n",
    "size = \"small\"\n",
    "\n",
    "args = {}\n",
    "args[\"rank\"] = 0\n",
    "# args[\"outdir\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts/pretraining_concatenation_LibriTTS-R/\"\n",
    "# args[\"outdir\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts/pretraining_addition_LibriTTS-R/\"\n",
    "args[\"outdir\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts/pretraining2_nocondition_LibriTTS-R_wavlmmel{'_'*int(bool(size))}{size}/\"\n",
    "\n",
    "input_type = \"wavlm\" if \"wavlm\" in args[\"outdir\"] else \"mel\"\n",
    "args[\"config_path\"] = f\"./../egs/l2-arctic/cascade/conf/{size}m2mvtn.wavlmmel_pt2.yaml\"\n",
    "# args[\"init_checkpoint\"] = f\"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts/pretraining_nocondition_LibriTTS-R_wavlmmel_small/checkpoint-300001steps.pkl\"\n",
    "args[\"init_checkpoint\"] = f\"\"\n",
    "args[\"resume\"] = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/AC_01/ckpts/pretraining2_nocondition_LibriTTS-R_wavlmmel_small/checkpoint-180000steps.pkl\"\n",
    "args[\"distributed\"] = False\n",
    "args = Dict2Obj(args)\n",
    "\n",
    "# load main config\n",
    "with open(args.config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "config.update(vars(args))\n",
    "\n",
    "# Customization\n",
    "config[\"model_params\"][\"conditiontype\"] = conditiontype\n",
    "config[\"optimizer_params\"][\"lr\"] = 0.00008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a73d4fbe-2fe1-4791-832a-8cbbc6e37984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Freezing decoder.embed.0.0.prenet.0.0.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.embed.0.0.prenet.0.0.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.embed.0.0.prenet.1.0.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.embed.0.0.prenet.1.0.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.embed.0.1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.embed.0.1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.embed.1.alpha. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.self_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.self_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.self_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.self_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.self_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.self_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.self_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.self_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.src_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.src_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.src_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.src_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.src_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.src_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.src_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.src_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.feed_forward.w_1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.feed_forward.w_1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.feed_forward.w_2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.feed_forward.w_2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.norm1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.norm1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.norm2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.norm2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.norm3.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.0.norm3.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.self_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.self_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.self_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.self_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.self_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.self_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.self_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.self_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.src_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.src_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.src_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.src_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.src_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.src_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.src_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.src_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.feed_forward.w_1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.feed_forward.w_1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.feed_forward.w_2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.feed_forward.w_2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.norm1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.norm1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.norm2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.norm2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.norm3.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.1.norm3.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.self_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.self_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.self_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.self_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.self_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.self_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.self_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.self_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.src_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.src_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.src_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.src_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.src_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.src_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.src_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.src_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.feed_forward.w_1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.feed_forward.w_1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.feed_forward.w_2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.feed_forward.w_2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.norm1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.norm1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.norm2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.norm2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.norm3.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.2.norm3.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.self_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.self_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.self_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.self_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.self_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.self_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.self_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.self_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.src_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.src_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.src_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.src_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.src_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.src_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.src_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.src_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.feed_forward.w_1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.feed_forward.w_1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.feed_forward.w_2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.feed_forward.w_2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.norm1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.norm1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.norm2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.norm2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.norm3.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.3.norm3.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.self_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.self_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.self_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.self_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.self_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.self_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.self_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.self_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.src_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.src_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.src_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.src_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.src_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.src_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.src_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.src_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.feed_forward.w_1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.feed_forward.w_1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.feed_forward.w_2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.feed_forward.w_2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.norm1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.norm1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.norm2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.norm2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.norm3.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.4.norm3.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.self_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.self_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.self_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.self_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.self_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.self_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.self_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.self_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.src_attn.linear_q.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.src_attn.linear_q.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.src_attn.linear_k.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.src_attn.linear_k.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.src_attn.linear_v.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.src_attn.linear_v.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.src_attn.linear_out.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.src_attn.linear_out.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.feed_forward.w_1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.feed_forward.w_1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.feed_forward.w_2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.feed_forward.w_2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.norm1.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.norm1.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.norm2.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.norm2.bias. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.norm3.weight. It will not be updated during training.\n",
      "WARNING:root:Freezing decoder.decoders.5.norm3.bias. It will not be updated during training.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_device(args.rank)\n",
    "if not os.path.exists(args.outdir):\n",
    "    os.makedirs(args.outdir)\n",
    "    \n",
    "### Dataset Preparation ###\n",
    "dataset = {\n",
    "    \"train\": PretrainingL2Arctic(dataset_dir, speakers, accents, genders, datasplit, scaler, \"train\", [\"wavlm\", \"mel\"]),\n",
    "    \"dev\": PretrainingL2Arctic(dataset_dir, speakers, accents, genders, datasplit, scaler, \"valid\", [\"wavlm\", \"mel\"]),\n",
    "}\n",
    "\n",
    "collater_class = getattr(\n",
    "    seq2seq_vc.collaters,\n",
    "    config.get(\"collater_type\", \"ARM2MVCCollater\"),\n",
    ")\n",
    "collater = collater_class()\n",
    "\n",
    "sampler = {\"train\": None, \"dev\": None}\n",
    "data_loader = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=dataset[\"train\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"train\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "    \"dev\": DataLoader(\n",
    "        dataset=dataset[\"dev\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=collater,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        sampler=sampler[\"dev\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "    ),\n",
    "}\n",
    "\n",
    "### Model Preparation ###\n",
    "model_class = getattr(\n",
    "    seq2seq_vc.models,\n",
    "    config.get(\"model_type\", \"M2MVTNPro\"),\n",
    ")\n",
    "model = model_class(**config[\"model_params\"]).to(device)\n",
    "\n",
    "if config.get(\"criterions\", None):\n",
    "    criterion = {\n",
    "        criterion_class: getattr(seq2seq_vc.losses, criterion_class)(\n",
    "            **criterion_paramaters\n",
    "        )\n",
    "        for criterion_class, criterion_paramaters in config[\"criterions\"].items()\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"Please specify criterions in the config file.\")\n",
    "\n",
    "### optimizers and schedulers ###\n",
    "optimizer_class = getattr(\n",
    "    torch.optim,\n",
    "    # keep compatibility\n",
    "    config.get(\"optimizer_type\", \"Adam\"),\n",
    ")\n",
    "optimizer = optimizer_class(\n",
    "    model.parameters(),\n",
    "    **config[\"optimizer_params\"],\n",
    ")\n",
    "scheduler_class = scheduler_classes.get(config.get(\"scheduler_type\", \"warmuplr\"))\n",
    "scheduler = scheduler_class(\n",
    "    optimizer=optimizer,\n",
    "    **config[\"scheduler_params\"],\n",
    ")\n",
    "\n",
    "### define trainer ###\n",
    "trainer_class = getattr(\n",
    "    seq2seq_vc.trainers,\n",
    "    config.get(\"trainer_type\", \"ARM2MVCADVTrainer\"),\n",
    ")\n",
    "trainer = trainer_class(\n",
    "    steps=0,\n",
    "    epochs=0,\n",
    "    data_loader=data_loader,\n",
    "    sampler=sampler,\n",
    "    model=model,\n",
    "    vocoder=None,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# load pretrained parameters from checkpoint\n",
    "if len(args.init_checkpoint) != 0:\n",
    "    trainer.load_trained_modules(\n",
    "        args.init_checkpoint, init_mods=config[\"init-mods\"]\n",
    "    )\n",
    "\n",
    "# resume from checkpoint\n",
    "if len(args.resume) != 0:\n",
    "    trainer.load_checkpoint(args.resume)\n",
    "\n",
    "# freeze modules if necessary\n",
    "if config.get(\"freeze-mods\", None) is not None:\n",
    "    assert type(config[\"freeze-mods\"]) is list\n",
    "    trainer.freeze_modules(config[\"freeze-mods\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdc6c57-cf10-42d2-a759-b9f28a9a1811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train]:  60%|██████    | 180003/300000 [00:07<76:41:52,  2.30s/it] Traceback (most recent call last):\n",
      "  File \"/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:76\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# train one epoch\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# check whether training is finished\u001b[39;00m\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:134\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_steps_per_epoch, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# train one step\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/ar_m2mvc_adv.py:106\u001b[0m, in \u001b[0;36mARM2MVCADVTrainer._train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 106\u001b[0m \u001b[43mgen_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutdir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43msteps.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully saved checkpoint @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msteps.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Git/seq2seq-vc/notebooks/../seq2seq_vc/trainers/base.py:105\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(checkpoint_path)):\n\u001b[1;32m    104\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(checkpoint_path))\n\u001b[0;32m--> 105\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 379\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/serialization.py:496\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 496\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    498\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m*\u001b[39m storage\u001b[38;5;241m.\u001b[39melement_size()\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/storage.py:72\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/cuda/__init__.py:540\u001b[0m, in \u001b[0;36m_CudaBase.type\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;66;03m# We could use a Protocol here to tell mypy that self has `get_device` method\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# but it is only available in the typing module on Python >= 3.8\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# or on typing_extensions module on Python >= 3.6\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_device()):  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_CudaBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mntcephfs/lab_data/shoinoue/miniconda3/envs/cuhk/lib/python3.8/site-packages/torch/_utils.py:45\u001b[0m, in \u001b[0;36m_type\u001b[0;34m(self, dtype, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot cast dense tensor to sparse tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.run()\n",
    "finally:\n",
    "    trainer.save_checkpoint(\n",
    "        os.path.join(config[\"outdir\"], f\"checkpoint-{trainer.steps}steps.pkl\")\n",
    "    )\n",
    "    logging.info(f\"Successfully saved checkpoint @ {trainer.steps}steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e18b6b-2086-47bc-871a-0f2ca960f568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42bcec-48fc-423e-b9e1-b04208125853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
