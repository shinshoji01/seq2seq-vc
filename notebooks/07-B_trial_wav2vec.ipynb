{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "156a34a3-b3c2-475f-bdc7-5d859d259cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../cuhksz-phd/sho_util/pyfiles/\")\n",
    "from sound import play_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9abf66-8d5b-44d0-813b-5eec76cdc5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16000\n",
    "ckpt_path = \"/mntcephfs/lab_data/shoinoue/Models/trained_models/wavlm/WavLM-Large.pt\"\n",
    "path = \"/mntcephfs/lab_data/shoinoue/Dataset/L2-ARCTIC/HKK/wav/arctic_a0001.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06ef641c-66af-4fa3-8ca3-27f1ae5a8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(fname, fs=16000):\n",
    "    \"\"\" Load an audio file and return PCM along with the sample rate \"\"\"\n",
    "\n",
    "    wav, sr = sf.read(fname)\n",
    "    if sr!=fs:\n",
    "        wav, _ = librosa.load(fname, fs)\n",
    "        sf.write(\"temp.wav\", wav, fs, subtype=\"PCM_24\") # 書き込み\n",
    "        wav, sr = sf.read(\"temp.wav\")\n",
    "    assert sr == fs\n",
    "\n",
    "    return wav, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aecb1023-b9c3-43ec-9c4e-6bc1f435f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedWav2VecModel(nn.Module):\n",
    "    def __init__(self, fname):\n",
    "        super().__init__()\n",
    "\n",
    "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([fname])\n",
    "        model = model[0]\n",
    "        model.eval()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            z = self.model.feature_extractor(x)\n",
    "            if isinstance(z, tuple):\n",
    "                z = z[0]\n",
    "            c = self.model.feature_aggregator(z)\n",
    "        return z, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544067a1-9665-4faf-b892-3c1c11a710c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \"\"\" Lightweight wrapper around a fairspeech embedding model \"\"\"\n",
    "\n",
    "    def __init__(self, fname, gpu=0):\n",
    "        self.gpu = gpu\n",
    "        self.model = PretrainedWav2VecModel(fname).cuda(gpu)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = torch.from_numpy(x).float().cuda(self.gpu)\n",
    "        with torch.no_grad():\n",
    "            z, c = self.model(x.unsqueeze(0))\n",
    "\n",
    "        return z.squeeze(0).cpu().numpy(), c.squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bb305da-c837-491e-a26f-a89630d77794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Prediction(ckpt_path, 0)\n",
    "wav, sr = read_audio(path)\n",
    "# z, c = model(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5790bfb-1d9a-469f-9328-7234ad40497d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286c52b-8505-4294-88ad-4092601a0929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
