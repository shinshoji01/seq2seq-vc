{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353f98a-d50f-4343-a085-576374eb33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "CHUNK_SIZE = 1024\n",
    "headers = {\n",
    "    \"Accept\": \"audio/mpeg\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    # \"xi-api-key\": \"sk_7205b11e0ebbab711f1a8ea74f1801e601b536ffd7049a2b\", # old\n",
    "    \"xi-api-key\": \"sk_3ec0c10190d82a4fc3b11bf6c1e7fbe872877899c7fad2ca\", # new\n",
    "}\n",
    "\n",
    "def process_audio(text, audiofile, speaker_id):\n",
    "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{speaker_id}\"\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_multilingual_v2\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    with open(audiofile, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                \n",
    "textpath = \"../../Dataset/CMU-ARCTIC/transciptions.txt\"\n",
    "file1 = open(textpath, 'r')\n",
    "Lines = file1.readlines()\n",
    "transcriptions = {line.split(\" \")[1]: line.split('\"')[1] for line in Lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45eea8-31dc-41d0-b9bd-f6b1f694d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./PD-AST/\"\n",
    "voiceids = {\n",
    "    \"Adam\": \"pNInz6obpgDQGcFmaJgB\",\n",
    "    \n",
    "    # \"TNI\": \"MpmvzVG7V9x1NbkUPjVk\",\n",
    "    # \"RRBI\": \"Q1Pm5cykKaBq2v2HckOj\",\n",
    "    # \"HKK\": \"SX85hGCDoEZvc5fEmxgv\",\n",
    "    # \"SVBI\": \"YF1rYo6M2Yl40ZQnhmUX\",\n",
    "    \n",
    "    # old account\n",
    "    # \"ASI\": \"Dnq2rswSKxxLdUYE9ARt\",\n",
    "    # \"BDL\": \"dw501enouzsh9uIAnbAs\",\n",
    "    \n",
    "    # new account\n",
    "    # \"ASI\": \"mfXp2UGhYy9CVfhbR80T\",\n",
    "    # \"BDL\": \"22EbXCP4IsFd37zg8M2t\",\n",
    "    # \"SLT\": \"iuutCMpgpPl7aa4Zerxj\",\n",
    "    \n",
    "    # full prompt with 44.1kHz\n",
    "    \"ASI\": \"n7UG1mrDAGhkM6Jp4ad3\",\n",
    "    \"TNI\": \"7MSSIGwrn7niZHnmFZQ1\",\n",
    "    \"HKK\": \"9I8SKP1WTmBG3o8WyMt5\",\n",
    "    \"SLT\": \"Sg4hcrJYcxtUZzg3hQlC\",\n",
    "    \"BDL\": \"Z7R3b2X0NK1pfSp01AtA\",\n",
    "    \n",
    "    # only valid\n",
    "    # \"ASI\": \"NsrvuJ6F9bOsRfwed1ET\",\n",
    "    # \"SLT\": \"Zoy7f4m2oJCfP4Sph2F0\",\n",
    "    # \"HKK\": \"zxqygYvyeaBnXQjljDd4\",\n",
    "    \n",
    "    # valid + shuffle\n",
    "    # \"ASI\": \"via1ElYhAo4KUwwgdsss\", # 0.2\n",
    "    # \"ASI\": \"3nywFZs7cpC9maGEoA8Z\", # 0.5\n",
    "    # \"ASI\": \"Uo7THUW6T8evBkKwCQ2Y\", # 1.0\n",
    "    \n",
    "    # wer\n",
    "    # \"ASI\": \"ynnlZ86VAOmtW6jYxGFx\",\n",
    "}\n",
    "\n",
    "files = glob.glob(\"./LLM_responses/08-A_multi-lingual_text/*\")\n",
    "# files = glob.glob(\"./LLM_responses/08-A_multi-lingual_text GPT3-Turbo/*\")\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d6afa-4515-48e3-a61b-825622f029e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = glob.glob(\"./LLM_responses/08-A_multi-lingual_text/*\")\n",
    "files1.sort()\n",
    "files2 = glob.glob(\"./LLM_responses/08-A_multi-lingual_text GPT3-Turbo/*\")\n",
    "files2.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a1eec-c0c7-444f-8b58-b9d3f60e834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files1)):\n",
    "    path1 = files1[i]\n",
    "    path2 = files2[i]\n",
    "    if not('arctic_a0024' in path1):\n",
    "        continue\n",
    "    try:\n",
    "        data = np.load(path1, allow_pickle=True).item()\n",
    "        text1 = data[\"Korean\"]\n",
    "        data = np.load(path2, allow_pickle=True).item()\n",
    "        text2 = data[\"Korean\"]\n",
    "        print(text1)\n",
    "        print(text2)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c7e70-21bb-4a29-b2ca-51ec83ce9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speakers = [\"HKK\"]\n",
    "speakers = [\"SLT\"]\n",
    "# speakers = [\"ASI\"]\n",
    "# speakers = [\"TNI\", \"HKK\"]\n",
    "# accents = [\"Hindi\"]\n",
    "# accents = [\"English\", \"Hindi\", \"Japanese\", \"Korean\"]\n",
    "# accents = [\"Korean\"]\n",
    "accents = [\"Korean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f29ae9-3135-4a28-a484-56a3878cb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "for spk in speakers:\n",
    "    spkid = voiceids[spk]\n",
    "    spk_dir = base_dir + f\"{spk}/\"\n",
    "    os.makedirs(spk_dir, exist_ok=True)\n",
    "    # for acc in accents:\n",
    "    for acc in accents:\n",
    "        acc_dir = spk_dir + f\"{acc}/\"\n",
    "        os.makedirs(acc_dir, exist_ok=True)\n",
    "        for path in tqdm(files):\n",
    "            savefile = acc_dir + os.path.basename(path)[:-4] + \".wav\"\n",
    "            if os.path.exists(savefile):\n",
    "                continue\n",
    "            data = np.load(path, allow_pickle=True).item()\n",
    "            text = data[acc]\n",
    "            if save:\n",
    "                process_audio(text, savefile, spkid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3bb44-7330-4f6c-b5ca-e306db1c24e0",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345adf32-2fb2-406a-9072-6fa66d3a53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import librosa\n",
    "import torchaudio\n",
    "import IPython\n",
    "import shutil\n",
    "def play_audio(data, rate):\n",
    "    IPython.display.display(IPython.display.Audio(data=data,rate=rate))\n",
    "    \n",
    "import sys\n",
    "sys.path.append(\"../cuhksz-phd/sho_util/pyfiles/\")\n",
    "from get_mel_spectrogram import audio2mel\n",
    "from basic import plot_spectrogram\n",
    "\n",
    "fs = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcb8fe-530e-4df4-8477-1e285f3ca3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spk = \"ASI\"\n",
    "# spk = \"HKK\"\n",
    "spk = \"SLT\"\n",
    "spk_dir = base_dir + f\"{spk}/\"\n",
    "accents = [\"English\", \"Hindi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f2fbb-3b45-4f75-b367-819b050f1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = [os.path.basename(a) for a in glob.glob(spk_dir + \"Hindi/*\")]\n",
    "baselines.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d6a58-cf62-4b51-8843-ac8cf1b721f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_spk_dir = f\"./audiosamples/elevenlabs/dataset/{spk}/\"\n",
    "copy = False\n",
    "os.makedirs(save_spk_dir, exist_ok=True)\n",
    "# for bl in baselines:\n",
    "for bl in np.random.choice(baselines, 10, False):\n",
    "    path = f\"./LLM_responses/08-A_multi-lingual_text/{bl[:-4]}.npy\"\n",
    "    data = np.load(path, allow_pickle=True).item()\n",
    "    print(\"English: \",data[\"English\"])\n",
    "    # for acc in np.array(accents)[np.random.choice(np.arange(2), 2, False)]:\n",
    "    # for acc in accents:\n",
    "    for acc in accents[1:]:\n",
    "        path = f\"{spk_dir}{acc}/{bl}\"\n",
    "        if copy:\n",
    "            tgt = save_spk_dir + f\"{acc}_{bl}\"\n",
    "            shutil.copyfile(path, tgt)\n",
    "        x, sr = torchaudio.load(path)\n",
    "        x = np.array(x)[0]\n",
    "        print(acc, \": \", data[acc])\n",
    "        # print(acc, sr)\n",
    "        play_audio(x, sr)\n",
    "    if spk in [\"SLT\"]:\n",
    "        path = f\"/Users/shoahoshoaho/Git/Dataset/CMU-ARCTIC/cmu_us_{spk.lower()}_arctic/wav/{bl}\"\n",
    "    else:\n",
    "        path = f\"/Users/shoahoshoaho/Git/Dataset/L2-ARCTIC_v5/{spk}/wav/{bl}\"\n",
    "    x, sr = torchaudio.load(path)\n",
    "    if copy:\n",
    "        tgt = save_spk_dir + f\"groundtruth_{bl}\"\n",
    "        shutil.copyfile(path, tgt)\n",
    "    x = np.array(x)[0]\n",
    "    # print(\"ground truth\", sr)\n",
    "    # play_audio(x, sr)\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf4f2a-156a-4534-9c67-cafe7e038cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483ef3c-f857-4752-b0de-b85090e7f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "# path = \"./PD-AST/ASI/English/arctic_a0002.wav\"\n",
    "# path = \"/Users/shoahoshoaho/Git/Dataset/L2-ARCTIC_v5/ASI/wav/arctic_a0002.wav\"\n",
    "path = \"/Users/shoahoshoaho/Git/Dataset/CMU-ARCTIC/cmu_us_slt_arctic/wav/arctic_a0001.wav\"\n",
    "x, _ = torchaudio.load(path)\n",
    "x = np.array(x)[0]\n",
    "# x, _ = librosa.load(\"./PD-AST/ASI/Hindi/arctic_a0002.wav\", fs)\n",
    "play_audio(x, _)\n",
    "stft = np.log(np.abs(librosa.stft(x, )))\n",
    "plot_spectrogram(stft, fig, (1, 2, 1), title=\"new\")\n",
    "\n",
    "# path = \"./PD-AST_old/ASI/English/arctic_a0002.wav\"\n",
    "# path = \"/Users/shoahoshoaho/Git/Dataset/L2-ARCTIC_v5/ASI/wav/arctic_a0002.wav\"\n",
    "# path = \"./audiosamples/l2-ARCTIC/ASI_0.wav\"\n",
    "# path = \"/Users/shoahoshoaho/Git/Dataset/CMU-ARCTIC/cmu_us_slt_arctic/wav/arctic_a0001.wav\"\n",
    "path = \"./PD-AST/SLT/English/arctic_a0001.wav\"\n",
    "x, _ = torchaudio.load(path)\n",
    "resampler = torchaudio.transforms.Resample(_, 16000, dtype=x.dtype)\n",
    "x = resampler(x)\n",
    "x = np.array(x)[0]\n",
    "# x, _ = librosa.load(\"./PD-AST_old/ASI/Hindi/arctic_a0002.wav\", fs)\n",
    "play_audio(x, 16000)\n",
    "stft = np.log(np.abs(librosa.stft(x)))\n",
    "plot_spectrogram(stft, fig, (1, 2, 2), title=\"old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a972bb-c3ae-42bf-82f3-65e5527d4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449679f-af30-4cb1-a046-c33de74f5d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
