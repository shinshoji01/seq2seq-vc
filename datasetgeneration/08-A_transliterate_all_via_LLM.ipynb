{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56f425-3576-4cd9-a951-63725402131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/openai/whisper.git \n",
    "# !pip install openai==1.35.13 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0e3de-3987-478e-98f5-57f2cd5724a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from phonemizer import phonemize\n",
    "import random\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "\n",
    "textpath = \"../../Dataset/CMU-ARCTIC/transciptions.txt\"\n",
    "file1 = open(textpath, 'r')\n",
    "Lines = file1.readlines()\n",
    "transcriptions = {line.split(\" \")[1]: line.split('\"')[1] for line in Lines}\n",
    "\n",
    "# pip install openai\n",
    "from openai import OpenAI\n",
    "api_key = \"sk-ANuLBmSVZUkf8I308b17C775Ce464c7dA06dF119E9A29fE3\"\n",
    "api_base = \"https://api.ai-gaochao.cn/v1\"\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "# def get_response(text):\n",
    "#     completion = client.chat.completions.create(\n",
    "#       # model=\"gpt-3.5-turbo-1106\",\n",
    "#       model=\"gpt-4o-2024-05-13\",\n",
    "#       messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": text}\n",
    "#       ]\n",
    "#     )\n",
    "#     return completion.choices[0].message.content\n",
    "\n",
    "def gpt_api_no_stream(prompt: str, \n",
    "                      # model='gpt-4o-2024-05-13',\n",
    "                      model='gpt-3.5-turbo-1106',\n",
    "                      reset_messages: bool = True,\n",
    "                      response_only: bool = True\n",
    "                      ):\n",
    "    \n",
    "    messages = [{'role': 'user','content': prompt},]\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "        )\n",
    "        completion = dict(completion)\n",
    "        msg = None\n",
    "        choices = completion.get('choices', None)\n",
    "        if choices:\n",
    "            msg = choices[0].message.content\n",
    "        else:\n",
    "            msg = completion.message.content\n",
    "    except Exception as err:\n",
    "        return (False, f'OpenAI API error: {err}')\n",
    "    if reset_messages:\n",
    "        messages.pop(-1)\n",
    "    else:\n",
    "        # add text of response to messages\n",
    "        messages.append({\n",
    "            'role': choices[0].message.role,\n",
    "            'content': choices[0].message.content\n",
    "        })\n",
    "    if response_only:\n",
    "        return True, msg\n",
    "    else:\n",
    "        return True, messages\n",
    "\n",
    "def GetLLMPrompt(sentence, language, phonemized=None):\n",
    "    words = sentence.split()\n",
    "    if type(phonemized)!=list:\n",
    "        phonemized = [phonemize(word, language='en-us', backend='espeak', with_stress=True).split()[0] for word in words]\n",
    "    shfflephonemized = phonemized\n",
    "\n",
    "    start = f\"\"\"Can you provide me with three {language} words to represent the phoneme sequences delimited by triple backticks. \n",
    "For example, in Japanese, \"Trail (tɹˈeɪl)\" is expected to have Japanese representation of \"トレイル\"; where \"'\" in phonemes represents the stress point of the word. \n",
    "Here, your task is to provide me with three {language} words that can replace the phoneme senquences, delimited by triple backticks.\n",
    "Please focus on phonetically similar characters instead of similar characters in terms of the meaning.\n",
    "The expected output should be in JSON format. \n",
    "You can first list three possible choices of the words and then re-order them in order of the similarity of the pronunciation. \n",
    "The following is the example in Hindi language.\n",
    "{{\n",
    "  \"I\": {{\n",
    "    \"phonemes\": \"ˈaɪ\",\n",
    "    \"choices\": [\"आई\", \"ऐ\", \"आई\"],\n",
    "    \"similarity order\": [\"आई\", \"ऐ\", \"आई\"]\n",
    "  }},\n",
    "  \"love\": {{\n",
    "    \"phonemes\": \"lˈʌv\",\n",
    "    \"choices\": [\"लव\", \"लव\", \"लव\"],\n",
    "    \"similarity order\": [\"लव\", \"लव\", \"लव\"]\n",
    "  }},\n",
    "  \"you\": {{\n",
    "    \"phonemes\": \"juː\",\n",
    "    \"choices\": [\"यू\", \"यू\", \"यू\"],\n",
    "    \"similarity order\": [\"यू\", \"यू\", \"यू\"]\n",
    "  }},\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "    for p, ph in enumerate(shfflephonemized):\n",
    "        start += f\"{words[p]}: {ph}\\n\"\n",
    "    start = start[:-1]\n",
    "    start += f\"\"\"\n",
    "```\n",
    "Again, the responses should be in a JSON format and sort them in order of the similarity to each phoneme sequence.\n",
    "{{\n",
    "\"\"\"\n",
    "    for p, ph in enumerate(shfflephonemized):\n",
    "        start += f\"\"\"  \"{words[p]}\": {{\n",
    "\"phonemes\": \"{ph}\",\n",
    "\"choices\": [`1st choices of {language} characters`, `2nd choices of {language} characters`, `3rd choices of {language} characters`],\n",
    "\"similarity order\": [`1st most similar {language} characters`, `2nd most similar {language} characters`, `3rd most similar {language} characters`],\n",
    "}},\\n\"\"\"\n",
    "    start = start[:] + \"}\"\n",
    "    return start\n",
    "\n",
    "adds = {\n",
    "    \"zhi\": [\"the\", [\"ðɪ\"]],\n",
    "    \"za\": [\"the pineapple\", [\"ðə\", \"pˈaɪnæpəl\"]],\n",
    "    \"ah\": [\"a little awkward\", [\"ɐ\",\"lˈɪɾəl\",\"ˈɔːkwɚd\"]],\n",
    "}\n",
    "\n",
    "# Evaluate each word\n",
    "postprocessing = {a: {} for a in adds}\n",
    "for addname in adds:\n",
    "# for addname in [\"zhi\"]:\n",
    "    sentence, phonemized = adds[addname]\n",
    "    # for language in [\"Hindi\", \"Korean\", \"Japanese\", \"Russian\"]:\n",
    "    for language in [\"Hindi\", \"Korean\"]:\n",
    "    # for language in [\"Hindi\", \"Korean\", \"Japanese\"]:\n",
    "        filelists = glob.glob(f\"./LLM_responses/08-A/{language}/postprocessing_{addname}_*.npy\")\n",
    "        a_list = []\n",
    "        for path in filelists:\n",
    "            response = np.load(path).item()\n",
    "            try:\n",
    "                a_list += [eval(response[response.index(\"{\"):-1*response[::-1].index(\"}\")]) if response[-1]!=\"}\" else eval(response[response.index(\"{\"):])]\n",
    "            except ValueError:\n",
    "                pass\n",
    "        # print(f\"{len(a_list)} / {len(filelists)}\")\n",
    "        # print(\"Normalized    :\", sentence)\n",
    "        dirs = []\n",
    "        for a in a_list:\n",
    "            a = {key: a[key] for key in sentence.split()}\n",
    "            dirs += [a]\n",
    "        for i in range(len(dirs)):\n",
    "            for key in dirs[i]:\n",
    "                newlist = []\n",
    "                for j in range(len(dirs[i][key][\"similarity order\"])):\n",
    "                    newlist += [dirs[i][key][\"similarity order\"][j]]*(3-j)\n",
    "                dirs[i][key][\"similarity order\"] = newlist\n",
    "        data = {key: [element for i in range(len(dirs)) for element in dirs[i][key][\"similarity order\"]] for key in dirs[0]}\n",
    "        # Get the transliterated sentences\n",
    "        arrays = []\n",
    "        counts = []\n",
    "        for word in sentence.split():\n",
    "            c = collections.Counter(data[word])\n",
    "            df = pd.DataFrame(c.items(), columns=[\"phonemes\", \"count\"]).sort_values(\"count\", ascending=False).values\n",
    "            arrays += [df[0,0]]\n",
    "            counts += [list(df[:1,1])]\n",
    "            \n",
    "        postprocessing[addname][language] = arrays[0]\n",
    "\n",
    "import re\n",
    "from whisper.normalizers.english import EnglishNumberNormalizer, EnglishSpellingNormalizer, remove_symbols_and_diacritics\n",
    "\n",
    "# keep numbers, -, and '\n",
    "class EnglishTextNormalizer:\n",
    "    def __init__(self):\n",
    "        self.ignore_patterns = r\"\\b(hmm|mm|mhm|mmm|uh|um)\\b\"\n",
    "        self.replacers = {\n",
    "            # common contractions\n",
    "            r\"\\bwon't\\b\": \"will not\",\n",
    "            r\"\\bcan't\\b\": \"can not\",\n",
    "            r\"\\blet's\\b\": \"let us\",\n",
    "            r\"\\bain't\\b\": \"aint\",\n",
    "            r\"\\by'all\\b\": \"you all\",\n",
    "            r\"\\bwanna\\b\": \"want to\",\n",
    "            r\"\\bgotta\\b\": \"got to\",\n",
    "            r\"\\bgonna\\b\": \"going to\",\n",
    "            r\"\\bi'ma\\b\": \"i am going to\",\n",
    "            r\"\\bimma\\b\": \"i am going to\",\n",
    "            r\"\\bwoulda\\b\": \"would have\",\n",
    "            r\"\\bcoulda\\b\": \"could have\",\n",
    "            r\"\\bshoulda\\b\": \"should have\",\n",
    "            r\"\\bma'am\\b\": \"madam\",\n",
    "            # contractions in titles/prefixes\n",
    "            r\"\\bmr\\b\": \"mister \",\n",
    "            r\"\\bmrs\\b\": \"missus \",\n",
    "            r\"\\bst\\b\": \"saint \",\n",
    "            r\"\\bdr\\b\": \"doctor \",\n",
    "            r\"\\bprof\\b\": \"professor \",\n",
    "            r\"\\bcapt\\b\": \"captain \",\n",
    "            r\"\\bgov\\b\": \"governor \",\n",
    "            r\"\\bald\\b\": \"alderman \",\n",
    "            r\"\\bgen\\b\": \"general \",\n",
    "            r\"\\bsen\\b\": \"senator \",\n",
    "            r\"\\brep\\b\": \"representative \",\n",
    "            r\"\\bpres\\b\": \"president \",\n",
    "            r\"\\brev\\b\": \"reverend \",\n",
    "            r\"\\bhon\\b\": \"honorable \",\n",
    "            r\"\\basst\\b\": \"assistant \",\n",
    "            r\"\\bassoc\\b\": \"associate \",\n",
    "            r\"\\blt\\b\": \"lieutenant \",\n",
    "            r\"\\bcol\\b\": \"colonel \",\n",
    "            r\"\\bjr\\b\": \"junior \",\n",
    "            r\"\\bsr\\b\": \"senior \",\n",
    "            r\"\\besq\\b\": \"esquire \",\n",
    "            # prefect tenses, ideally it should be any past participles, but it's harder..\n",
    "            r\"'d been\\b\": \" had been\",\n",
    "            r\"'s been\\b\": \" has been\",\n",
    "            r\"'d gone\\b\": \" had gone\",\n",
    "            r\"'s gone\\b\": \" has gone\",\n",
    "            r\"'d done\\b\": \" had done\",  # \"'s done\" is ambiguous\n",
    "            r\"'s got\\b\": \" has got\",\n",
    "            # general contractions\n",
    "            r\"n't\\b\": \" not\",\n",
    "            r\"'re\\b\": \" are\",\n",
    "            # r\"'s\\b\": \" is\",\n",
    "            r\"'d\\b\": \" would\",\n",
    "            r\"'ll\\b\": \" will\",\n",
    "            r\"'t\\b\": \" not\",\n",
    "            r\"'ve\\b\": \" have\",\n",
    "            r\"'m\\b\": \" am\",\n",
    "        }\n",
    "        self.standardize_numbers = EnglishNumberNormalizer()\n",
    "        self.standardize_spellings = EnglishSpellingNormalizer()\n",
    "\n",
    "    def __call__(self, s: str):\n",
    "        s = s.lower()\n",
    "\n",
    "        s = re.sub(r\"[<\\[][^>\\]]*[>\\]]\", \"\", s)  # remove words between brackets\n",
    "        s = re.sub(r\"\\(([^)]+?)\\)\", \"\", s)  # remove words between parenthesis\n",
    "        s = re.sub(self.ignore_patterns, \"\", s)\n",
    "        # s = re.sub(r\"\\s+'\", \"'\", s)  # when there's a space before an apostrophe\n",
    "\n",
    "        for pattern, replacement in self.replacers.items():\n",
    "            s = re.sub(pattern, replacement, s)\n",
    "\n",
    "        s = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", s)  # remove commas between digits\n",
    "        s = re.sub(r\"\\.([^0-9]|$)\", r\" \\1\", s)  # remove periods not followed by numbers\n",
    "        s = remove_symbols_and_diacritics(s, keep=\".%$¢€£-'\")  # keep numeric symbols\n",
    "\n",
    "        # s = self.standardize_numbers(s)\n",
    "        s = self.standardize_spellings(s)\n",
    "\n",
    "        # now remove prefix/suffix symbols that are not preceded/followed by numbers\n",
    "        s = re.sub(r\"[.$¢€£]([^0-9])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"([^0-9])%\", r\"\\1 \", s)\n",
    "\n",
    "        s = re.sub(r\"\\s+\", \" \", s)  # replace any successive whitespaces with a space\n",
    "        \n",
    "        return s\n",
    "normalizer = EnglishTextNormalizer()\n",
    "\n",
    "def get_original_sentence(basename):\n",
    "    original_sentence = transcriptions[basename]\n",
    "    original_sentence = original_sentence.replace(\" --\", \",\")\n",
    "    return original_sentence\n",
    "\n",
    "# obrien = []\n",
    "# for b, basename in enumerate(transcriptions):\n",
    "#     original_sentence = get_original_sentence(basename)\n",
    "#     sentence = normalizer(original_sentence)\n",
    "#     # if len(sentence.split())!=len(original_sentence.split()):\n",
    "#     candidates = [\"I'm\", \"'ll\", \"'re\", \"'s\", \"'t\", \"'ve\", \"'d\"]\n",
    "#     if np.array([key in original_sentence for key in candidates]).sum()==0:\n",
    "#         if \"-\" in original_sentence:\n",
    "#             print(original_sentence)\n",
    "#             print(sentence)\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981a98e-b7cc-4d37-aede-a151b58ac262",
   "metadata": {},
   "source": [
    "- Get responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e6cb4-3a4a-4219-99e6-c781641626d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatnum = 3\n",
    "save = False\n",
    "fin = []\n",
    "# for language in [\"Hindi\", \"Korean\", \"Japanese\"]:\n",
    "for language in [\"Korean\"]:\n",
    "    print(language)\n",
    "    # for key in tqdm(list(transcriptions.keys())):\n",
    "    for key in [\"arctic_a0176\"]:\n",
    "        # if key in allerrors:\n",
    "        #     pass\n",
    "        # else:\n",
    "        #     if len(glob.glob(f\"./LLM_responses/08-A/{language}/{key}_*.npy\"))>=repeatnum:\n",
    "        #         continue\n",
    "        exist_length = len(glob.glob(f\"./LLM_responses/08-A/{language}/{key}_*.npy\"))\n",
    "        if exist_length>=repeatnum:\n",
    "            continue\n",
    "        else:\n",
    "            start = exist_length\n",
    "        osentence = get_original_sentence(key)\n",
    "        sentence = normalizer(osentence)\n",
    "        if save:\n",
    "            prompt = GetLLMPrompt(sentence, language)\n",
    "        for r in range(start, repeatnum):\n",
    "            savepath = f\"./LLM_responses/08-A/{language}/{key}_{r}.npy\"\n",
    "            if save:\n",
    "                try:\n",
    "                    # response = get_response(prompt)\n",
    "                    response = gpt_api_no_stream(prompt)[1]\n",
    "                except AuthenticationError:\n",
    "                    continue\n",
    "                np.save(savepath, response)\n",
    "                fin += [key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5140ebe-405b-4742-b6d5-cff27f8f95f3",
   "metadata": {},
   "source": [
    "- Get the possible sentence from responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00c59b-e2f8-440f-b4d5-f200834e2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd80ea-9380-4eac-9085-925ab9b2a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# languages = [\"Hindi\", \"Korean\"]\n",
    "# languages = [\"Hindi\", \"Korean\"]\n",
    "languages = [\"Hindi\", \"Korean\"]\n",
    "# languages = [\"Hindi\", \"Korean\", \"Japanese\"]\n",
    "# languages = [\"Hindi\", \"Korean\", \"Japanese\", \"Russian\"]\n",
    "errors = []\n",
    "added = []\n",
    "for b, basename in enumerate(list(transcriptions.keys())):\n",
    "    \n",
    "    # savepath = f\"./LLM_responses/08-A_multi-lingual_text/{basename}.npy\"\n",
    "    savepath = f\"./LLM_responses/08-A_multi-lingual_text GPT3-Turbo/{basename}.npy\"\n",
    "    try:\n",
    "        a = np.load(savepath, allow_pickle=True).item()\n",
    "        if np.array([la in a for la in languages+[\"English\", \"Original English\"]]).mean()==1:\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    # if basename!=\"arctic_a0016\":\n",
    "    #     continue\n",
    "    try:\n",
    "        original_sentence = get_original_sentence(basename)\n",
    "        sentence = normalizer(original_sentence)\n",
    "\n",
    "        sentences = {}\n",
    "        for language in languages:\n",
    "            rank = 1\n",
    "\n",
    "            # Evaluate each word\n",
    "            filelists = []\n",
    "            filelists += glob.glob(f\"./LLM_responses/08-A/{language}/{basename}_*.npy\")[:3]\n",
    "            # filelists += glob.glob(f\"./LLM_responses/08-A GPT3-Turbo/{language}/{basename}_*.npy\")\n",
    "            filelists += glob.glob(f\"./LLM_responses/08-A GPT3-Turbo/{language}/{basename}_*.npy\")[:3]\n",
    "            assert len(filelists)>=6\n",
    "            \n",
    "            a_list = []\n",
    "            for path in filelists:\n",
    "                response = np.load(path).item()\n",
    "                try:\n",
    "                    a = eval(response[response.index(\"{\"):-1*response[::-1].index(\"}\")]) if response[-1]!=\"}\" else eval(response[response.index(\"{\"):])\n",
    "\n",
    "                    if len(a)==len(set(sentence.split())):\n",
    "                        # Check whether the response is valid\n",
    "                        test = []\n",
    "                        for word in sentence.split():\n",
    "                            exist = word in a\n",
    "                            if not(exist):\n",
    "                                normalized_word = normalizer.standardize_numbers(word)\n",
    "                                a_array = np.array(list(a.keys()))\n",
    "                                bl = normalized_word==a_array\n",
    "                                exist = bool(bl.sum())\n",
    "                                if exist:\n",
    "                                    a[word] = a[normalized_word]\n",
    "                            ## check the type of data\n",
    "                            if exist:\n",
    "                                if type(a[word])!=dict:\n",
    "                                    exist = False\n",
    "                            test += [exist]\n",
    "                        if np.array([test]).mean()==1:\n",
    "                            a_list += [a]\n",
    "                except (ValueError, SyntaxError) as e:\n",
    "                    pass\n",
    "            dirs = []\n",
    "            for a in a_list:\n",
    "                a = {key: a[key] for key in sentence.split()}\n",
    "                dirs += [a]\n",
    "            ordernames = []\n",
    "            for i in range(len(dirs)):\n",
    "                for key in dirs[i]:\n",
    "                    newlist = []\n",
    "                    try:\n",
    "                        ordername = \"similarity order\"\n",
    "                        dirs[i][key][ordername]\n",
    "                    except KeyError:\n",
    "                        ordername = \"similarity_order\"\n",
    "                    # delete duplicated words\n",
    "                    candidates = list(set(dirs[i][key][ordername]))\n",
    "                    newwords = []\n",
    "                    for tword in dirs[i][key][ordername]:\n",
    "                        if tword in candidates:\n",
    "                            candidates.remove(tword)\n",
    "                            newwords += [tword]\n",
    "                        if len(candidates)==0:\n",
    "                            break\n",
    "                    dirs[i][key][ordername] = newwords\n",
    "                    for j in range(len(dirs[i][key][ordername])):\n",
    "                        newlist += [dirs[i][key][ordername][j]]*(len(newwords)-j)\n",
    "                        # newlist += [dirs[i][key][ordername][j]]\n",
    "                    dirs[i][key][ordername] = newlist\n",
    "                ordernames += [ordername]\n",
    "            data = {key: [element for i in range(len(dirs)) for element in dirs[i][key][ordernames[i]]] for key in dirs[0]}\n",
    "\n",
    "            # Get the transliterated sentences\n",
    "            arrays = []\n",
    "            words = sentence.split()\n",
    "            for w, word in enumerate(words):\n",
    "                if word in list(set([a.split(\" \")[0] for a in np.array(list(adds.values()))[:, 0]])):\n",
    "                    if word==\"a\":\n",
    "                        arrays += [postprocessing[\"ah\"][language]]\n",
    "                    if word==\"the\":\n",
    "                        pro = phonemize(words[w] + \" \" + words[w+1], language='en-us', backend='espeak', with_stress=True).split()[0]\n",
    "                        for the in [\"zhi\", \"za\"]:\n",
    "                            if adds[the][1][0]==pro:\n",
    "                                break\n",
    "                        arrays += [postprocessing[the][language]]\n",
    "                else:\n",
    "                    c = collections.Counter(data[word])\n",
    "                    df = pd.DataFrame(c.items(), columns=[\"phonemes\", \"count\"]).sort_values(\"count\", ascending=False).values\n",
    "                    arrays += [df[0,0]]\n",
    "\n",
    "            # put period and comma\n",
    "            try:\n",
    "                targets = [\".\", \",\"]\n",
    "                now = 0\n",
    "                english_arrays = sentence.split()\n",
    "                for word in original_sentence.split():\n",
    "                # for word in sentence.split():\n",
    "                    normalized = normalizer(word)\n",
    "                    num = len(normalized.split(\" \"))\n",
    "                    now += (num-1)\n",
    "                    for target in targets:\n",
    "                        if target in word:\n",
    "                            arrays[now] += target\n",
    "                            english_arrays[now] += target\n",
    "                    now += 1\n",
    "            except IndexError:\n",
    "                errors += [basename]\n",
    "                continue\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(f\"{b+1} / {len(transcriptions)}: {basename}\")\n",
    "            print(f\"{len(a_list)} / {len(filelists)}\")\n",
    "            print(\"English       :\", original_sentence)\n",
    "            print(\"Normalized    :\", sentence)\n",
    "            transliterated = \" \".join(arrays)\n",
    "            print(\"Transliterated: \", transliterated)\n",
    "\n",
    "            print()\n",
    "            sentences[\"Original English\"] = original_sentence\n",
    "            sentences[\"English\"] = \" \".join(english_arrays)\n",
    "            sentences[language] = transliterated\n",
    "\n",
    "            for i in range(len(data)):\n",
    "                c = collections.Counter(data[list(data.keys())[i]])\n",
    "                print(list(data.keys())[i], \"  \", str(c)[8:-1])\n",
    "    except IndexError:\n",
    "        added += [basename]\n",
    "    np.save(savepath, sentences)\n",
    "    # if b>-1:\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f29658-8274-4917-ac1b-9b8eb221d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"./LLM_responses/08-A_multi-lingual_text/*\")\n",
    "files.sort()\n",
    "\n",
    "path = files[4]\n",
    "print(path)\n",
    "a = np.load(path, allow_pickle=True).item()\n",
    "for key in a:\n",
    "    print(a[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe69d22-8b78-4c7d-a344-98ea205d685a",
   "metadata": {},
   "source": [
    "# VCTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc870ef-aa86-4226-8e66-98abdebf7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "vctktexts = np.load(\"VCTK_transcriptions.npy\", allow_pickle=True).item()\n",
    "for key in vctktexts:\n",
    "    text = vctktexts[key]\n",
    "    num += len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee0744-62b6-4782-ace9-48022830e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatnum = 3\n",
    "save = True\n",
    "fin = []\n",
    "vctktexts = np.load(\"VCTK_transcriptions.npy\", allow_pickle=True).item()\n",
    "# for language in [\"Hindi\", \"Korean\", \"Japanese\"]:\n",
    "for language in [\"Hindi\"]:\n",
    "    print(language)\n",
    "    for key in tqdm(list(vctktexts.keys())[:1500]):\n",
    "        # exist_length = len(glob.glob(f\"./LLM_responses/08-VCTK/{language}/{key}_*.npy\"))\n",
    "        exist_length = len(glob.glob(f\"./LLM_responses/08-VCTK GPT3-Turbo/{language}/{key}_*.npy\"))\n",
    "        if exist_length>=repeatnum:\n",
    "            continue\n",
    "        else:\n",
    "            start = exist_length\n",
    "        osentence = vctktexts[key]\n",
    "        sentence = normalizer(osentence)\n",
    "        if save:\n",
    "            prompt = GetLLMPrompt(sentence, language)\n",
    "        for r in range(start, repeatnum):\n",
    "            savepath = f\"./LLM_responses/08-VCTK GPT3-Turbo/{language}/{key}_{r}.npy\"\n",
    "            if save:\n",
    "                try:\n",
    "                    # response = get_response(prompt)\n",
    "                    response = gpt_api_no_stream(prompt)[1]\n",
    "                except AuthenticationError:\n",
    "                    continue\n",
    "                np.save(savepath, response)\n",
    "                fin += [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d63e3-7e13-433b-a2f3-fb3e62a92dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"Hindi\"]\n",
    "errors = []\n",
    "added = []\n",
    "for b, basename in enumerate(list(vctktexts.keys())[:20]):\n",
    "    savepath = f\"./LLM_responses/08-VCTK_multi-lingual_text/{basename}.npy\"\n",
    "    try:\n",
    "        a = np.load(savepath, allow_pickle=True).item()\n",
    "        if np.array([la in a for la in languages+[\"English\", \"Original English\"]]).mean()==1:\n",
    "            continue\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    # if basename!=\"arctic_a0016\":\n",
    "    #     continue\n",
    "    try:\n",
    "        original_sentence = vctktexts[basename]\n",
    "        sentence = normalizer(original_sentence)\n",
    "\n",
    "        sentences = {}\n",
    "        for language in languages:\n",
    "            rank = 1\n",
    "\n",
    "            # Evaluate each word\n",
    "            filelists = []\n",
    "            filelists += glob.glob(f\"./LLM_responses/08-VCTK/{language}/{basename}_*.npy\")[:3]\n",
    "            # filelists += glob.glob(f\"./LLM_responses/08-VCTK GPT3-Turbo/{language}/{basename}_*.npy\")[:3]\n",
    "            assert len(filelists)>=3\n",
    "            \n",
    "            a_list = []\n",
    "            for path in filelists:\n",
    "                response = np.load(path).item()\n",
    "                try:\n",
    "                    a = eval(response[response.index(\"{\"):-1*response[::-1].index(\"}\")]) if response[-1]!=\"}\" else eval(response[response.index(\"{\"):])\n",
    "\n",
    "                    if len(a)==len(set(sentence.split())):\n",
    "                        # Check whether the response is valid\n",
    "                        test = []\n",
    "                        for word in sentence.split():\n",
    "                            exist = word in a\n",
    "                            if not(exist):\n",
    "                                normalized_word = normalizer.standardize_numbers(word)\n",
    "                                a_array = np.array(list(a.keys()))\n",
    "                                bl = normalized_word==a_array\n",
    "                                exist = bool(bl.sum())\n",
    "                                if exist:\n",
    "                                    a[word] = a[normalized_word]\n",
    "                            ## check the type of data\n",
    "                            if exist:\n",
    "                                if type(a[word])!=dict:\n",
    "                                    exist = False\n",
    "                            test += [exist]\n",
    "                        if np.array([test]).mean()==1:\n",
    "                            a_list += [a]\n",
    "                except (ValueError, SyntaxError) as e:\n",
    "                    pass\n",
    "            dirs = []\n",
    "            for a in a_list:\n",
    "                a = {key: a[key] for key in sentence.split()}\n",
    "                dirs += [a]\n",
    "            ordernames = []\n",
    "            for i in range(len(dirs)):\n",
    "                for key in dirs[i]:\n",
    "                    newlist = []\n",
    "                    try:\n",
    "                        ordername = \"similarity order\"\n",
    "                        dirs[i][key][ordername]\n",
    "                    except KeyError:\n",
    "                        ordername = \"similarity_order\"\n",
    "                    # delete duplicated words\n",
    "                    candidates = list(set(dirs[i][key][ordername]))\n",
    "                    newwords = []\n",
    "                    for tword in dirs[i][key][ordername]:\n",
    "                        if tword in candidates:\n",
    "                            candidates.remove(tword)\n",
    "                            newwords += [tword]\n",
    "                        if len(candidates)==0:\n",
    "                            break\n",
    "                    dirs[i][key][ordername] = newwords\n",
    "                    for j in range(len(dirs[i][key][ordername])):\n",
    "                        newlist += [dirs[i][key][ordername][j]]*(len(newwords)-j)\n",
    "                        # newlist += [dirs[i][key][ordername][j]]\n",
    "                    dirs[i][key][ordername] = newlist\n",
    "                ordernames += [ordername]\n",
    "            data = {key: [element for i in range(len(dirs)) for element in dirs[i][key][ordernames[i]]] for key in dirs[0]}\n",
    "\n",
    "            # Get the transliterated sentences\n",
    "            arrays = []\n",
    "            words = sentence.split()\n",
    "            for w, word in enumerate(words):\n",
    "                if word in list(set([a.split(\" \")[0] for a in np.array(list(adds.values()))[:, 0]])):\n",
    "                    if word==\"a\":\n",
    "                        arrays += [postprocessing[\"ah\"][language]]\n",
    "                    if word==\"the\":\n",
    "                        pro = phonemize(words[w] + \" \" + words[w+1], language='en-us', backend='espeak', with_stress=True).split()[0]\n",
    "                        for the in [\"zhi\", \"za\"]:\n",
    "                            if adds[the][1][0]==pro:\n",
    "                                break\n",
    "                        arrays += [postprocessing[the][language]]\n",
    "                else:\n",
    "                    c = collections.Counter(data[word])\n",
    "                    df = pd.DataFrame(c.items(), columns=[\"phonemes\", \"count\"]).sort_values(\"count\", ascending=False).values\n",
    "                    arrays += [df[0,0]]\n",
    "\n",
    "            # put period and comma\n",
    "            try:\n",
    "                targets = [\".\", \",\"]\n",
    "                now = 0\n",
    "                english_arrays = sentence.split()\n",
    "                for word in original_sentence.split():\n",
    "                # for word in sentence.split():\n",
    "                    normalized = normalizer(word)\n",
    "                    num = len(normalized.split(\" \"))\n",
    "                    now += (num-1)\n",
    "                    for target in targets:\n",
    "                        if target in word:\n",
    "                            arrays[now] += target\n",
    "                            english_arrays[now] += target\n",
    "                    now += 1\n",
    "            except IndexError:\n",
    "                errors += [basename]\n",
    "                continue\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(f\"{b+1} / {len(vctktexts)}: {basename}\")\n",
    "            print(f\"{len(a_list)} / {len(filelists)}\")\n",
    "            print(\"English       :\", original_sentence)\n",
    "            print(\"Normalized    :\", sentence)\n",
    "            transliterated = \" \".join(arrays)\n",
    "            print(\"Transliterated: \", transliterated)\n",
    "\n",
    "            print()\n",
    "            sentences[\"Original English\"] = original_sentence\n",
    "            sentences[\"English\"] = \" \".join(english_arrays)\n",
    "            sentences[language] = transliterated\n",
    "\n",
    "            for i in range(len(data)):\n",
    "                c = collections.Counter(data[list(data.keys())[i]])\n",
    "                print(list(data.keys())[i], \"  \", str(c)[8:-1])\n",
    "    except IndexError:\n",
    "        added += [basename]\n",
    "    np.save(savepath, sentences)\n",
    "    # if b>-1:\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6447cbc-88bf-4608-9ec2-246bf2bb58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"./LLM_responses/08-VCTK_multi-lingual_text/*\")\n",
    "files.sort()\n",
    "\n",
    "path = files[4]\n",
    "print(path)\n",
    "a = np.load(path, allow_pickle=True).item()\n",
    "for key in a:\n",
    "    print(a[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cd686-4ae1-4e71-902d-b3355d73e9fc",
   "metadata": {},
   "source": [
    "# Train, Val, Test Split for ARCTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c63a13-9b88-4608-a198-df9a5c43ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = 932, 100, 100\n",
    "np.random.seed(0)\n",
    "randomized = np.random.choice(np.array(list(transcriptions.keys())), 1132, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75280c26-f8aa-4282-8cb5-406cc767d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = list(np.sort(randomized[:train]))\n",
    "valset = list(np.sort(randomized[train:train+val]))\n",
    "testset = list(np.sort(randomized[-test:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fec1b-8573-44c2-92d5-f01b4d1e7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./data_split_ARCTIC.npy\", [trainset, valset, testset])\n",
    "# list(np.load(\"./data_split_ARCTIC.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0a903-fdf7-4d44-b4e0-30f16a3d7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers_error = [\n",
    "#     'arctic_a0031',\n",
    "#     'arctic_a0210',\n",
    "#     'arctic_a0245',\n",
    "#     'arctic_a0276',\n",
    "#     'arctic_a0348',\n",
    "#     'arctic_a0371',\n",
    "#     'arctic_a0434',\n",
    "#     'arctic_a0453',\n",
    "#     'arctic_a0556',\n",
    "#     'arctic_b0211',\n",
    "#     'arctic_b0236',\n",
    "#     'arctic_b0260',\n",
    "#     'arctic_b0330',\n",
    "#     'arctic_b0348',\n",
    "#     'arctic_b0365',\n",
    "#     'arctic_b0526',\n",
    "# ]\n",
    "# obrien_error = [\n",
    "#     'arctic_a0252',\n",
    "#     'arctic_a0493',\n",
    "#     'arctic_a0494',\n",
    "#     'arctic_a0574',\n",
    "#     'arctic_b0447',\n",
    "# ]\n",
    "# dashsymbol_error =  [\n",
    "#     'arctic_a0006',\n",
    "#     'arctic_a0016',\n",
    "#     'arctic_a0047',\n",
    "#     'arctic_a0049',\n",
    "#     'arctic_a0154',\n",
    "#     'arctic_a0162',\n",
    "#     'arctic_a0189',\n",
    "#     'arctic_a0204',\n",
    "#     'arctic_a0213',\n",
    "#     'arctic_a0266',\n",
    "#     'arctic_a0298',\n",
    "#     'arctic_a0313',\n",
    "#     'arctic_a0320',\n",
    "#     'arctic_a0321',\n",
    "#     'arctic_a0324',\n",
    "#     'arctic_a0325',\n",
    "#     'arctic_a0330',\n",
    "#     'arctic_a0331',\n",
    "#     'arctic_a0335',\n",
    "#     'arctic_a0337',\n",
    "#     'arctic_a0478',\n",
    "#     'arctic_a0484',\n",
    "#     'arctic_a0491',\n",
    "#     'arctic_a0514',\n",
    "#     'arctic_a0570',\n",
    "#     'arctic_b0231',\n",
    "#     'arctic_b0271',\n",
    "#     'arctic_b0292',\n",
    "#     'arctic_b0294',\n",
    "#     'arctic_b0298',\n",
    "#     'arctic_b0300',\n",
    "#     'arctic_b0361',\n",
    "#     'arctic_b0379',\n",
    "#     'arctic_b0397',\n",
    "#     'arctic_b0421',\n",
    "#     'arctic_b0425',\n",
    "#     'arctic_b0428',\n",
    "#     'arctic_b0429',\n",
    "#     'arctic_b0446',\n",
    "#     'arctic_b0501',\n",
    "# ]\n",
    "# including_s_error = [\n",
    "#     'arctic_a0015',\n",
    "#     'arctic_a0016',\n",
    "#     'arctic_a0034',\n",
    "#     'arctic_a0056',\n",
    "#     'arctic_a0062',\n",
    "#     'arctic_a0072',\n",
    "#     'arctic_a0073',\n",
    "#     'arctic_a0083',\n",
    "#     'arctic_a0084',\n",
    "#     'arctic_a0132',\n",
    "#     'arctic_a0140',\n",
    "#     'arctic_a0162',\n",
    "#     'arctic_a0166',\n",
    "#     'arctic_a0180',\n",
    "#     'arctic_a0226',\n",
    "#     'arctic_a0241',\n",
    "#     'arctic_a0269',\n",
    "#     'arctic_a0284',\n",
    "#     'arctic_a0297',\n",
    "#     'arctic_a0320',\n",
    "#     'arctic_a0324',\n",
    "#     'arctic_a0350',\n",
    "#     'arctic_a0374',\n",
    "#     'arctic_a0378',\n",
    "#     'arctic_a0381',\n",
    "#     'arctic_a0385',\n",
    "#     'arctic_a0394',\n",
    "#     'arctic_a0399',\n",
    "#     'arctic_a0451',\n",
    "#     'arctic_a0459',\n",
    "#     'arctic_a0509',\n",
    "#     'arctic_a0556',\n",
    "#     'arctic_a0563',\n",
    "#     'arctic_a0566',\n",
    "#     'arctic_a0581',\n",
    "#     'arctic_a0583',\n",
    "#     'arctic_a0585',\n",
    "#     'arctic_a0592',\n",
    "#     'arctic_b0005',\n",
    "#     'arctic_b0011',\n",
    "#     'arctic_b0027',\n",
    "#     'arctic_b0076',\n",
    "#     'arctic_b0106',\n",
    "#     'arctic_b0113',\n",
    "#     'arctic_b0117',\n",
    "#     'arctic_b0120',\n",
    "#     'arctic_b0123',\n",
    "#     'arctic_b0126',\n",
    "#     'arctic_b0162',\n",
    "#     'arctic_b0193',\n",
    "#     'arctic_b0208',\n",
    "#     'arctic_b0209',\n",
    "#     'arctic_b0219',\n",
    "#     'arctic_b0221',\n",
    "#     'arctic_b0231',\n",
    "#     'arctic_b0260',\n",
    "#     'arctic_b0262',\n",
    "#     'arctic_b0299',\n",
    "#     'arctic_b0312',\n",
    "#     'arctic_b0334',\n",
    "#     'arctic_b0346',\n",
    "#     'arctic_b0351',\n",
    "#     'arctic_b0354',\n",
    "#     'arctic_b0384',\n",
    "#     'arctic_b0419',\n",
    "#     'arctic_b0496',\n",
    "#     'arctic_b0528',\n",
    "# ]\n",
    "# godbless_error = [\n",
    "#     \"arctic_a0006\",\n",
    "# ]\n",
    "# allerrors = list(set(numbers_error + obrien_error + dashsymbol_error + including_s_error + godbless_error))\n",
    "# allerrors.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f2ade-a334-4ebb-8259-43eeb11d7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Can you provide me with three Japanese words to represent the phoneme sequences delimited by triple backticks. For example, in Japanese, ``Trail (\\\\textipa{t\\\\textturnr\"eIl})'' is expected to have Japanese representation of ``\\\\begin{CJK}{UTF8}{min}トレイル''\\end{CJK}; where ``''' in phonemes represents the stress point of the word. Here, your task is to provide me with three Japanese words that can replace the phoneme senquences, delimited by triple backticks. Please focus on phonetically similar characters instead of similar characters in terms of the meaning. The expected output should be in JSON format. You can first list three possible choices of the words and then re-order them in order of the similarity of the pronunciation. \n",
    "The following is the example in Hindi language.\n",
    "$[$Few Shot Examples$]$\n",
    "\n",
    "\\\\textasciigrave\\\\textasciigrave\\\\textasciigrave\n",
    "Let's: \\\\textipa{l\"Ets} \\\\\n",
    "go: \\\\textipa{g\"oU}\n",
    "\\\\textasciigrave\\\\textasciigrave\\\\textasciigrave\n",
    "Again, the responses should be in a JSON format and sort them in order of the similarity to each phoneme sequence.\n",
    "\\{\n",
    "  ``Let's'': \\{\n",
    "``phonemes'': ``\\\\textipa{l\"Ets}'',\n",
    "``choices'': $[$\\\\textasciigrave 1st choices of Japanese characters\\\\textasciigrave , \\\\textasciigrave 2nd choices of Japanese characters\\\\textasciigrave , \\\\textasciigrave 3rd choices of Japanese characters\\\\textasciigrave $]$,\n",
    "``similarity order'': $[$\\\\textasciigrave 1st most similar Japanese characters\\\\textasciigrave , \\\\textasciigrave 2nd most similar Japanese characters\\\\textasciigrave , \\\\textasciigrave 3rd most similar Japanese characters\\\\textasciigrave $]$,\n",
    "\\},\n",
    "  ``go'': \\{\n",
    "``phonemes'': ``\\\\textipa{g\"oU}'',\n",
    "``choices'': $[$\\\\textasciigrave 1st choices of Japanese characters\\\\textasciigrave , \\\\textasciigrave 2nd choices of Japanese characters\\\\textasciigrave , \\\\textasciigrave 3rd choices of Japanese characters\\\\textasciigrave $]$,\n",
    "``similarity order'': $[$\\\\textasciigrave 1st most similar Japanese characters\\\\textasciigrave , \\\\textasciigrave 2nd most similar Japanese characters\\\\textasciigrave , \\\\textasciigrave 3rd most similar Japanese characters\\\\textasciigrave $]$,\n",
    "\\},\n",
    "\\}\n",
    "\"\"\"\n",
    "for l in text.split(\"\\n\")[1:-1]:\n",
    "    if l!=\"\":\n",
    "        print(l + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f91c8e-a5d1-407b-8744-e8047bf4279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\\{\n",
    "  ``Let's'': \\{\n",
    "    ``phonemes'': ``\\\\textipa{l\"Ets}'',\n",
    "    ``choices'': [``\\\\begin{CJK}{UTF8}{min}レッツ\\end{CJK}'', ``\\\\begin{CJK}{UTF8}{min}レツ\\end{CJK}'', ``\\\\begin{CJK}{UTF8}{min}レテス\\end{CJK}''],\n",
    "    ``similarity order'': [``\\\\begin{CJK}{UTF8}{min}レッツ\\end{CJK}'', ``\\\\begin{CJK}{UTF8}{min}レツ\\end{CJK}'', ``\\\\begin{CJK}{UTF8}{min}レテス\\end{CJK}'']\n",
    "  \\},\n",
    "  ``go'': \\{\n",
    "    ``phonemes'': ``\\\\textipa{g\"oU}'',\n",
    "    ``choices'': [``\\\\begin{CJK}{UTF8}{min}ゴー\\end{CJK}'', ``\\\\begin{CJK}{UTF8}{min}ゴウ\\end{CJK}'', ``\\\\begin{CJK}{UTF8}{min}ゴ\\end{CJK}''],\n",
    "    ``similarity order'': [``\\\\begin{CJK}{UTF8}{min}ゴー\\end{CJK}'', ``\\\\begin{CJK}{UTF8}{min}ゴウ\\end{CJK}'', ``\\\\begin{CJK}{UTF8}{min}ゴ\\end{CJK}'']\n",
    "  \\}\n",
    "\\}\n",
    "\"\"\"\n",
    "for l in text.split(\"\\n\")[1:-1]:\n",
    "    if l!=\"\":\n",
    "        print(l + \"\\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
